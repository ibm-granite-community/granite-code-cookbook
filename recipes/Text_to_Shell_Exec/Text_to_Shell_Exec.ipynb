{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate and Execute a Bash Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook builds on the [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb) recipe. Here, the generated Bash code is executed as a system command.\n",
    "\n",
    "> **WARNING:** This recipe executes code generated by language models. The generated code may delete or modify files on your system. Use with caution!\n",
    "\n",
    "See the [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb) recipe for instructions on installing the models you will need. It also has information on concepts like _system prompts_ and the differences between commands on different operating systems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "\n",
    "Granite Kitchen comes with a bundle of dependencies that are required for Granite Cookbook recipes. See the list of packages in its [`setup.py`](https://github.com/ibm-granite-community/granite-kitchen/blob/main/setup.py). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/granite-kitchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select a model\n",
    "\n",
    "Select a Granite Code model from the [`ibm-granite`](https://replicate.com/ibm-granite) org on Replicate. \n",
    "Recall that using a smaller model, like `3b`, usually results in generated code that may not be valid for your machine. Use the `20b` model if you can, as shown below.\n",
    "\n",
    "Here we use the Replicate Langchain client to connect to the model. To connect to a model on a provider other than Replicate, substitute this code cell with one from the [LLM component recipe](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Components/Langchain_LLMs.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "model = Replicate(\n",
    "    model=\"ibm-granite/granite-20b-code-instruct-8k\",\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect your operating system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb), we used a Python helper function to determine the host operating system, MacOS, Linux, etc. We used this information in the prompts to encourage the model to generate shell commands that work on the host system, because some shell commands differ between operating systems. However, the output may still contain commands that are not supported by your operating system. Because this notebook attempts to run the generated commands, you may see failures if an incorrect command syntax was generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, platform\n",
    "\n",
    "def os_name():\n",
    "    os_name = platform.system()\n",
    "    # It turns out, using \"MacOS\" is better than \"Darwin\", which is what gets returned on MacOS.\n",
    "    # For all other cases, the returned value should be fine as is, so we map the result to the desired\n",
    "    # name, but only for MacOS...\n",
    "    name_map = {'Darwin': 'MacOS'}\n",
    "    shell_map = {'Windows': 'DOS'} # On Windows and use Power Shell, change from `DOS` to `Power Shell`.\n",
    "    # ... then pass the os_name value as the second arg, which is used as the default return value.\n",
    "    # For the shell name, return `bash` by default. (You can change this to zsh, fish, etc.)\n",
    "    return name_map.get(os_name, os_name), shell_map.get(os_name, 'bash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_os, my_shell = os_name()\n",
    "my_os, my_shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing the Prompt for Direct Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the System Prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First we want to modify the _system prompt_ we used in the previous notebook to work better for our purposes.\n",
    "\n",
    "Writing prompts is an art. Recall in [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb), our output was usually Markdown with quoted sections of shell code and commentary explaining how it worked. Here, we just want code output that we execute without editing. Here are a few tips on writing prompts for our purposes here:\n",
    "\n",
    "> **TIPS:**\n",
    ">\n",
    "> 1. Rather than use a question (\"How do I ...?\") in a prompt, provide a directive (\"Write a script that ...\"). This helps prevent the model from generating dialogue around the code.\n",
    "> 2. Add instructions to the system prompt like this: \"You are a helpful software engineer. You write clear, concise, well-commented code. You only print valid code. You don't print any commentary about the code nor markdown syntax to wrap the code.\"\n",
    "\n",
    "So here is our new system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "system_prompt = SystemMessage(content=dedent(f\"\"\"\\\n",
    "    You are a helpful software engineer. You write clear, concise,\n",
    "    well-commented code. You only print valid code. You don't print\n",
    "    any commentary about the code nor do you wrap the code in markdown syntax!\n",
    "    You make sure you only generate {my_shell} code that is {my_os}-compatible!\n",
    "    Do not output anything but the code.\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide a list of Q&A examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the examples uses the `stat` command, which requires different syntax for Linux vs. MacOS systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_flags = '-c \"%y %n\" {}'\n",
    "if my_os == 'MacOS':\n",
    "    stat_flags = '-f \"%m %N\" {}'\n",
    "print(f\"The 'stat' flags for my OS \\'{my_os}\\' and \\'{my_shell}\\' are \\'{stat_flags}\\'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE:** If you are using a Windows system, try changing the \"answers\" in the `examples` cell to be valid Power Shell or DOS commands. You can ignore the `stat_flags` in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"question\": \"Recursively find files that match '*.js', and filter out files with 'excludeddir' in their paths.\", \n",
    "        \"answer\": \"find . -name '*.js' | grep -v excludeddir\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Dump \\\"a0b\\\" as hexadecimal bytes.\", \n",
    "        \"answer\": \"printf \\\"a0b\\\" | od -tx1\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Create a tar ball of all pdf files in the current folder and any subdirectories.\", \n",
    "        \"answer\": \"find . -name '*.pdf' | xargs tar czvf pdf.tar\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Sort all files and directories in the current directory, but no subdirectories, according to modification time, and print only the seven most recently modified items.\", \n",
    "        \"answer\": f\"find . -maxdepth 1 -exec stat {stat_flags} \\; | sort -n -r | tail -n 7\",\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Find all the empty directories in and under the current directory.\", \n",
    "        \"answer\": \"find . -type d -empty\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the prompt template\n",
    "\n",
    "Here we build up a chat prompt template from messages. See the [Langchain docs](https://python.langchain.com/docs/how_to/few_shot_examples_chat/#fixed-examples) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{question}\"),\n",
    "        (\"ai\", \"{answer}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_prompt,\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(chat_template.input_variables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the completed prompt\n",
    "\n",
    "Create a prompt and inspect the fully-interpolated chat template. Alternating Human/AI messages create a structure that the model will follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_prompt1 = dedent(f\"\"\"\\\n",
    "    Write a {my_shell} script to print the first 50 files found under the current working directory\n",
    "    that have been modified within the last week. Make sure you show the last modification time\n",
    "    for each file in the output.\"\"\").replace(\"\\n\", \"\")\n",
    "\n",
    "print(chat_template.format(question=shell_prompt1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model - example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | model\n",
    "shell_code1 = chain.invoke({\"question\": shell_prompt1})\n",
    "print(shell_code1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this output to what you got in the [../Text_to_Shell](../Text_to_Shell/Text_to_Shell.ipynb) recipe. Is this output a valid script and nothing else? Or, is there extra commentary and Markdown formatting? If you got this extra, undesirable output, try running the cell again. Does modifying the prompt or system prompt help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can attempt to execute the script! There is no need for an additional helper function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(shell_code1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model - example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_code2 = chain.invoke({\n",
    "    \"question\": f\"\"\"Write a {my_shell} script to recursively find Jupyter notebooks\n",
    "in the parent directory and print their paths.\"\"\"\n",
    "})\n",
    "\n",
    "print(shell_code2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(shell_code2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try invoking the model several times. How do the responses change from one invocation to the next? Try different queries. adding more examples to the `examples` string or modifying the ones shown. Does this affect the outputs?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
