{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Use Remote Granite Code Models (20B) with LangChain for Unit Test Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Introduction and Setup\n",
    " \n",
    "This recipe demonstrates how to generate unit tests for Python functions and classes using inference calls against a model hosted remotely on [Replicate](https://replicate.com/). This recipe targets developers who are looking to streamline the process of creating unit tests with minimal manual effort. The user inputs Python code and returns unit test code, incorporating \"test doubles\" for external dependencies.  The notebook depends on Granite [`Utils`](https://github.com/ibm-granite-community/utils) package for integration with LLMs using the [Langchain](https://www.langchain.com/) framework.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "To run this notebook, ensure you have the following:\n",
    "\n",
    "1. Python version: 3.9 or higher\n",
    "2. A Replicate API token. See the `../recipes/Getting_Started_with_Granite_Code.ipynb` for details.\n",
    "\n",
    "### Model Details:\n",
    "\n",
    "1. Model Platform : Replicate\n",
    "2. Model : IBM Granite 20b Code Instruct 8k\n",
    "3. Model Version : ibm-granite/granite-20b-code-instruct-8k:409a0c68b74df416c7ae2a3f1552101123356f5a2c6e46d681629b62904c605b\n",
    "\n",
    "### Program \n",
    "\n",
    "1. Input: Python code/snippets with instructions for test packages that need to utilized and optional type of unit test case scenarios to be covered.\n",
    "2. Output: Python code with unit test packages and libraries, test doubles, assert implementation for Unit testing of given input.\n",
    "\n",
    "> **Note:**\n",
    ">\n",
    "> Results using the 20b code instruct Granite model are generally better than the outputs when using the 8b code instruct Granite model. Whichever model you use, the code generated may require additional modifications to work, depending on the test libraries requested and other aspects of the user input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Install the required Langchain and Replicate packages\n",
    "\n",
    "Include a granite-community package with some simple utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    \"langchain_community<0.3.0\" \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import set_env_var, get_env_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Define a System Prompt\n",
    "\n",
    "We will pass the following system prompt as part of the inference call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Role: Python Code Generator.\n",
    "User Input: <Python code>, optional Test libraries, output file locations.\n",
    "Output: Python code for unit testing success and failure conditions of the given input <python code> leveraging the specified test libraries. \n",
    "Validity: Generates error-free unit test code for the input <python code> by importing those libraries.\n",
    "Test Libraries: User provided test libraries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Remote Model using Replicate\n",
    "\n",
    "We will use Granite code models hosted at [Replicate](https://replicate.com) for inference, hosted in the [ibm-granite](https://replicate.com/ibm-granite) organization.\n",
    "\n",
    "> **TIP:** If you get an \"authentication\" or similar error below, see the instructions mentioned above at `../recipes/Getting_Started_with_Granite_Code.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Now, we define the model to use and a dictional of parameters to pass to the `Replicate` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"ibm-granite/granite-20b-code-instruct-8k\"\n",
    " \n",
    "input_parameters = {      \n",
    "        \"top_k\": 60,\n",
    "        \"top_p\": 0.3, \n",
    "        \"max_tokens\": 1000,\n",
    "        \"min_tokens\": 0,\n",
    "        \"temperature\": 0.3, \n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"system_prompt\": system_prompt\n",
    "        }\n",
    "from langchain_community.llms import Replicate\n",
    "\n",
    "granite_via_replicate = Replicate(\n",
    "            model=model_id,\n",
    "            model_kwargs=input_parameters,\n",
    "            replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Perform Inference\n",
    "\n",
    "Next, we invoke the model to generate test cases for application code.\n",
    "\n",
    "The first example requests generation of unit-test code for the input Python code shown in the prompt. We specifically ask the model to use Python's `unittest` library for the test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "code1=\"\"\"\n",
    "Use Python's \"unittest\" library to generate unit tests for the following code:\n",
    "\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    if 'queryStringParameters' in event:    # If parameters\n",
    "        print(event['queryStringParameters']['first_name'])\n",
    "        print(event['queryStringParameters']['last_name'])\n",
    "        body = 'Hello {} {}!'.format(event['queryStringParameters']['first_name'], \n",
    "                                    event['queryStringParameters']['last_name'])  \n",
    "    else:    # If no parameters\n",
    "        print('No parameters!')\n",
    "        body = 'Who are you?'\n",
    "        \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(body)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code1)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Here are some steps you can use to try running the generated test code:\n",
    "\n",
    "1. Save the `lambda_handler` code in the prompt to a Python file. Include the import statements. Let's assume you name this file `lambda_handler.py`.\n",
    "2. Save the generated test code to a file, for example `test_lambda_handler.py`, in the same directory.\n",
    "\n",
    "You will most likely need to modify the input statement for importing `lambda_handler` that was generated for the test code. For example, if you followed our example naming convention and both files are in the same directory, then the import statement will be:\n",
    "\n",
    "```python\n",
    "from lambda_handler import lambda_handler\n",
    "```\n",
    "\n",
    "Now you can run the tests using the following shell command in the same directory with the files:\n",
    "\n",
    "```shell\n",
    "python -m unittest\n",
    "```\n",
    "\n",
    "Do the tests pass? How good are the tests themselves? Can you modify the prompt with suggestions for improving the quality of the tests. For example, what \"corner cases\" should the tests cover?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Second Example: Generate Tests for Multiple Functions\n",
    "\n",
    "Try running the output tests the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2=\"\"\"\n",
    "Use Python's \"unittest\" library to generate unit tests for the following code:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def load_data(fname):\n",
    "    points = np.loadtxt(fname, delimiter=',') \n",
    "    y_ = points[:,1]\n",
    "    # append '1' to account for the intercept\n",
    "    x_ = np.ones([len(y_),2]) \n",
    "    x_[:,0] = points[:,0]\n",
    "    # display plot\n",
    "    #plt.plot(x_[:,0], y_, 'ro')\n",
    "    #plt.xlabel('x-axis')\n",
    "    #plt.ylabel('y-axis')\n",
    "    #plt.show()\n",
    "    print('data loaded. x:{} y:{}'.format(x_.shape, y_.shape))\n",
    "    return x_, y_\n",
    "\n",
    "def evaluate_cost(x_,y_,params):\n",
    "    tempcost = 0\n",
    "    for i in range(len(y_)):\n",
    "        tempcost += (y_[i] - ((params[0] * x_[i,0]) + params[1])) ** 2 \n",
    "    return tempcost / float(10000)   \n",
    "\n",
    "def evaluate_gradient(x_,y_,params):\n",
    "    m_gradient = 0\n",
    "    b_gradient = 0\n",
    "    N = float(len(y_))\n",
    "    for i in range(len(y_)):\n",
    "        m_gradient += -(2/N) * (x_[i,0] * (y_[i] - ((params[0] * x_[i,0]) + params[1])))\n",
    "        b_gradient += -(2/N) * (y_[i] - ((params[0] * x_[i,0]) + params[1]))\n",
    "    return [m_gradient,b_gradient]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code2)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Third Example: Generate Tests for \"Middleware\" Code\n",
    "\n",
    "We'll also explicit ask for calls to other components to be replaced with \"mocks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "code3=\"\"\"\n",
    "Use the \"pytest\" library to generate unit tests for the following code. Use mocks and test data for the calls to Kafka:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create Spark session\n",
    "    spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\") # Ignore INFO DEBUG output\n",
    "    df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", topic_name) \\\n",
    "        .load()\n",
    "\n",
    "    df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "    df = df.withColumn(\"data\", from_json(df.value, Sentiment.get_schema())).select(\"data.*\")\n",
    "    df = df \\\n",
    "        .withColumn(\"ts\", to_timestamp(from_unixtime(expr(\"timestamp_ms/1000\")))) \\\n",
    "        .withWatermark(\"ts\", \"1 seconds\") # old data will be removed\n",
    "\n",
    "    # Preprocess the data\n",
    "    df = Sentiment.preprocessing(df)\n",
    "\n",
    "    # text classification to define polarity and subjectivity\n",
    "    df = Sentiment.text_classification(df)\n",
    "\n",
    "    assert type(df) == pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "    row_df = df.select(\n",
    "        to_json(struct(\"id\")).alias('key'),\n",
    "        to_json(struct('text', 'lang', 'ts', 'polarity_v', 'polarity', 'subjectivity_v')).alias(\"value\")\n",
    "    )\n",
    " \n",
    "\n",
    "    # Writing to Kafka\n",
    "    query = row_df\\\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "        .writeStream\\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"topic\", output_topic) \\\n",
    "        .option(\"checkpointLocation\", \"file:/Users/user/tmp\") \\\n",
    "        .start()\n",
    " \n",
    "    query.awaitTermination()\"\"\"\n",
    " \n",
    "replicate_response = granite_via_replicate.invoke(code3)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Fourth Example: More Use of \"Test Doubles\"\n",
    "\n",
    "Here is a scenario with the user input code contains a class definition, which is used to generate test code using test doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "code4=\"\"\"\n",
    "Use the \"pytest\" library to generate unit tests for the following class definition. Use test doubles and test data as appropriate to test this class:\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "class Sentiment:\n",
    "    def get_schema():\n",
    "        schema = StructType([\n",
    "            StructField(\"created_at\", StringType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"text\", StringType()),\n",
    "            StructField(\"source\", StringType()),\n",
    "            StructField(\"truncated\", StringType()),\n",
    "            StructField(\"in_reply_to_status_id\", StringType()),\n",
    "            StructField(\"in_reply_to_user_id\", StringType()),\n",
    "            StructField(\"in_reply_to_screen_name\", StringType()),\n",
    "            StructField(\"user\", StringType()),\n",
    "            StructField(\"coordinates\", StringType()),\n",
    "            StructField(\"place\", StringType()),\n",
    "            StructField(\"quoted_status_id\", StringType()),\n",
    "            StructField(\"is_quote_status\", StringType()),\n",
    "            StructField(\"quoted_status\", StringType()),\n",
    "            StructField(\"retweeted_status\", StringType()),\n",
    "            StructField(\"quote_count\", StringType()),\n",
    "            StructField(\"reply_count\", StringType()),\n",
    "            StructField(\"retweet_count\", StringType()),\n",
    "            StructField(\"favorite_count\", StringType()),\n",
    "            StructField(\"entities\", StringType()),\n",
    "            StructField(\"extended_entities\", StringType()),\n",
    "            StructField(\"favorited\", StringType()),\n",
    "            StructField(\"retweeted\", StringType()),\n",
    "            StructField(\"possibly_sensitive\", StringType()),\n",
    "            StructField(\"filter_level\", StringType()),\n",
    "            StructField(\"lang\", StringType()),\n",
    "            StructField(\"matching_rules\", StringType()),\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"timestamp_ms\", StringType())\n",
    "        ])\n",
    "        return schema\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocessing(df):\n",
    "        # words = df.select(explode(split(df.text, \" \")).alias(\"word\"))\n",
    "        df = df.filter(col('text').isNotNull())\n",
    "        df = df.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', r'[^\\x00-\\x7F]+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', r'[\\n\\r]', ' '))\n",
    "        df = df.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', '#', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', ':', ''))\n",
    "        df = df.withColumn('source', regexp_replace('source', '<a href=\"' , ''))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # text classification\n",
    "    @staticmethod\n",
    "    def polarity_detection(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "    @staticmethod\n",
    "    def subjectivity_detection(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "    @staticmethod\n",
    "    def text_classification(words):\n",
    "        # polarity detection\n",
    "        polarity_detection_udf = udf(Sentiment.polarity_detection, FloatType())\n",
    "        words = words.withColumn(\"polarity_v\", polarity_detection_udf(\"text\"))\n",
    "        words = words.withColumn(\n",
    "            'polarity',\n",
    "            when(col('polarity_v') > 0, lit('Positive'))\n",
    "            .when(col('polarity_v') == 0, lit('Neutral'))\n",
    "            .otherwise(lit('Negative'))\n",
    "        )\n",
    "        # subjectivity detection\n",
    "        subjectivity_detection_udf = udf(Sentiment.subjectivity_detection, FloatType())\n",
    "        words = words.withColumn(\"subjectivity_v\", subjectivity_detection_udf(\"text\"))\n",
    "        return words\n",
    "\n",
    "\"\"\"\n",
    "replicate_response = granite_via_replicate.invoke(code4)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Fifth Example: Test External API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "code5=\"\"\"\n",
    "Use the \"pytest\" library to generate unit tests for the following code. Use test doubles and test data as appropriate to test this class:\n",
    "\n",
    "import facebook\n",
    "\n",
    "token = 'your token'\n",
    "\n",
    "graph = facebook.GraphAPI(token)\n",
    "profile = graph.get_object(\"me\")\n",
    "friends = graph.get_connections(\"me\", \"friends\")\n",
    "friend_list = [friend['name'] for friend in friends['data']]\n",
    "print friend_list\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code5)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Sixth Example: Same as the Fifth Example, but Omit Requesting a Specific Test Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "code6=\"\"\"\n",
    "Generate unit tests for the following code. Use test doubles and test data as appropriate to test this class:\n",
    "\n",
    "import facebook\n",
    "\n",
    "token = 'your token'\n",
    "\n",
    "graph = facebook.GraphAPI(token)\n",
    "profile = graph.get_object(\"me\")\n",
    "friends = graph.get_connections(\"me\", \"friends\")\n",
    "friend_list = [friend['name'] for friend in friends['data']]\n",
    "print friend_list\"\"\"\n",
    "\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code6)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
