{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Use Remote Granite Code Models (20B) with LangChain for Unit Test Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Introduction and Setup\n",
    " \n",
    "This recipe demonstrates how to generate unit tests for Python functions and classes using inference calls against a model hosted remotely on [Replicate](https://replicate.com/). This recipe targets developers who are looking to streamline the process of creating unit tests with minimal manual effort. The user inputs Python code and returns unit test code, incorporating \"test doubles\" for external dependencies.  The notebook depends on Granite [`Utils`](https://github.com/ibm-granite-community/utils) package for integration with LLMs using the [Langchain](https://www.langchain.com/) framework.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "To run this notebook, ensure you have the following:\n",
    "\n",
    "1. Python version: 3.9 or higher\n",
    "2. A Replicate API token. See the `../recipes/Getting_Started_with_Granite_Code.ipynb` for details.\n",
    "\n",
    "### Model Details:\n",
    "\n",
    "1. Model Platform : Replicate\n",
    "2. Model : IBM Granite 20b Code Instruct 8k\n",
    "3. Model Version : ibm-granite/granite-20b-code-instruct-8k:409a0c68b74df416c7ae2a3f1552101123356f5a2c6e46d681629b62904c605b\n",
    "\n",
    "### Program \n",
    "\n",
    "1. Input: Python code/snippets with instructions for test packages that need to utilized and optional type of unit test case scenarios to be covered.\n",
    "2. Output: Python code with unit test packages and libraries, test doubles, assert implementation for Unit testing of given input.\n",
    "\n",
    "> **Note:**\n",
    ">\n",
    "> Results using the 20b code instruct Granite model are generally better than the outputs when using the 8b code instruct Granite model. Whichever model you use, the code generated may require additional modifications to work, depending on the test libraries requested and other aspects of the user input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Install the required Langchain and Replicate packages\n",
    "\n",
    "Include a granite-community package with some simple utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ibm-granite-community/utils\n",
      "  Cloning https://github.com/ibm-granite-community/utils to /private/var/folders/8t/m9m188_d0tb8szvfqlc20hfr0000gn/T/pip-req-build-idbityg4\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/ibm-granite-community/utils /private/var/folders/8t/m9m188_d0tb8szvfqlc20hfr0000gn/T/pip-req-build-idbityg4\n",
      "  Resolved https://github.com/ibm-granite-community/utils to commit a5965f40db3950dd2a41f3ca62a2c34adcdc20d7\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting langchain_community<0.3.0\n",
      "  Using cached langchain_community-0.2.19-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting replicate\n",
      "  Using cached replicate-1.0.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting python-dotenv (from ibm-granite-community-utils==0.1.dev46)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting PyYAML>=5.3 (from langchain_community<0.3.0)\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community<0.3.0)\n",
      "  Downloading SQLAlchemy-2.0.36-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.7 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community<0.3.0)\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community<0.3.0)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain<0.3.0,>=0.2.17 (from langchain_community<0.3.0)\n",
      "  Using cached langchain-0.2.17-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.43 (from langchain_community<0.3.0)\n",
      "  Using cached langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.112 (from langchain_community<0.3.0)\n",
      "  Using cached langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain_community<0.3.0)\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Collecting requests<3,>=2 (from langchain_community<0.3.0)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain_community<0.3.0)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting httpx<1,>=0.21.0 (from replicate)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: packaging in /Users/pravinkedia/Downloads/LatestCookBook/granite-code-cookbook/.venv/lib/python3.12/site-packages (from replicate) (24.2)\n",
      "Collecting pydantic>1.10.7 (from replicate)\n",
      "  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting typing_extensions>=4.5.0 (from replicate)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0)\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0)\n",
      "  Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0)\n",
      "  Downloading propcache-0.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community<0.3.0)\n",
      "  Downloading yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community<0.3.0)\n",
      "  Downloading marshmallow-3.23.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community<0.3.0)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting anyio (from httpx<1,>=0.21.0->replicate)\n",
      "  Using cached anyio-4.7.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting certifi (from httpx<1,>=0.21.0->replicate)\n",
      "  Downloading certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.21.0->replicate)\n",
      "  Using cached httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting idna (from httpx<1,>=0.21.0->replicate)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.21.0->replicate)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.17->langchain_community<0.3.0)\n",
      "  Using cached langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.43->langchain_community<0.3.0)\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.112->langchain_community<0.3.0)\n",
      "  Using cached orjson-3.10.12-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.112->langchain_community<0.3.0)\n",
      "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic>1.10.7->replicate)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic>1.10.7->replicate)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2->langchain_community<0.3.0)\n",
      "  Downloading charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain_community<0.3.0)\n",
      "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.43->langchain_community<0.3.0)\n",
      "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community<0.3.0)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting sniffio>=1.1 (from anyio->httpx<1,>=0.21.0->replicate)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Using cached langchain_community-0.2.19-py3-none-any.whl (2.3 MB)\n",
      "Using cached replicate-1.0.4-py3-none-any.whl (48 kB)\n",
      "Downloading aiohttp-3.11.11-cp312-cp312-macosx_11_0_arm64.whl (455 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Using cached langchain-0.2.17-py3-none-any.whl (1.0 MB)\n",
      "Using cached langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "Using cached langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading SQLAlchemy-2.0.36-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Downloading certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp312-cp312-macosx_11_0_arm64.whl (119 kB)\n",
      "Downloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Using cached langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading marshmallow-3.23.2-py3-none-any.whl (49 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Using cached orjson-3.10.12-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (248 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\n",
      "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl (92 kB)\n",
      "Using cached anyio-4.7.0-py3-none-any.whl (93 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Building wheels for collected packages: ibm-granite-community-utils\n",
      "  Building wheel for ibm-granite-community-utils (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ibm-granite-community-utils: filename=ibm_granite_community_utils-0.1.dev46-py3-none-any.whl size=8306 sha256=56f799a1e9f4c698f18592a70cb3b3786afaa4e5425a0a5d7c23a877d5b67884\n",
      "  Stored in directory: /private/var/folders/8t/m9m188_d0tb8szvfqlc20hfr0000gn/T/pip-ephem-wheel-cache-0djcc3o2/wheels/e2/74/0e/e7dc80cad1c61a0c57be9aff96c6c6bbb058052bc5b9cac0ff\n",
      "Successfully built ibm-granite-community-utils\n",
      "Installing collected packages: urllib3, typing_extensions, tenacity, sniffio, PyYAML, python-dotenv, propcache, orjson, numpy, mypy-extensions, multidict, marshmallow, jsonpointer, idna, h11, frozenlist, charset-normalizer, certifi, attrs, annotated-types, aiohappyeyeballs, yarl, typing-inspect, SQLAlchemy, requests, pydantic-core, jsonpatch, ibm-granite-community-utils, httpcore, anyio, aiosignal, requests-toolbelt, pydantic, httpx, dataclasses-json, aiohttp, replicate, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
      "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.36 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 annotated-types-0.7.0 anyio-4.7.0 attrs-24.3.0 certifi-2024.12.14 charset-normalizer-3.4.0 dataclasses-json-0.6.7 frozenlist-1.5.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 ibm-granite-community-utils-0.1.dev46 idna-3.10 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.17 langchain-core-0.2.43 langchain-text-splitters-0.2.4 langchain_community-0.2.19 langsmith-0.1.147 marshmallow-3.23.2 multidict-6.1.0 mypy-extensions-1.0.0 numpy-1.26.4 orjson-3.10.12 propcache-0.2.1 pydantic-2.10.4 pydantic-core-2.27.2 python-dotenv-1.0.1 replicate-1.0.4 requests-2.32.3 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-8.5.0 typing-inspect-0.9.0 typing_extensions-4.12.2 urllib3-2.3.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    \"langchain_community<0.3.0\" \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import set_env_var, get_env_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Define a System Prompt\n",
    "\n",
    "We will pass the following system prompt as part of the inference call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "Role: Python Code Generator.\n",
    "User Input: <Python code>, optional Test libraries, output file locations.\n",
    "Output: Python code for unit testing success and failure conditions of the given input <python code> leveraging the specified test libraries. \n",
    "Validity: Generates error-free unit test code for the input <python code> by importing those libraries.\n",
    "Test Libraries: User provided test libraries.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Remote Model using Replicate\n",
    "\n",
    "We will use Granite code models hosted at [Replicate](https://replicate.com) for inference, hosted in the [ibm-granite](https://replicate.com/ibm-granite) organization.\n",
    "\n",
    "> **TIP:** If you get an \"authentication\" or similar error below, see the instructions mentioned above at `../recipes/Getting_Started_with_Granite_Code.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Now, we define the model to use and a dictional of parameters to pass to the `Replicate` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"ibm-granite/granite-3.1-8b-instruct\"\n",
    " \n",
    "input_parameters = {      \n",
    "        \"top_k\": 60,\n",
    "        \"top_p\": 0.3, \n",
    "        \"max_tokens\": 1000,\n",
    "        \"min_tokens\": 0,\n",
    "        \"temperature\": 0.3, \n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"system_prompt\": system_prompt\n",
    "        }\n",
    "from langchain_community.llms import Replicate\n",
    "\n",
    "granite_via_replicate = Replicate(\n",
    "            model=model_id,\n",
    "            model_kwargs=input_parameters,\n",
    "            replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Perform Inference\n",
    "\n",
    "Next, we invoke the model to generate test cases for application code.\n",
    "\n",
    "The first example requests generation of unit-test code for the input Python code shown in the prompt. We specifically ask the model to use Python's `unittest` library for the test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granite response from Replicate: ```python\n",
      "import unittest\n",
      "import json\n",
      "from main import lambda_handler  # Assuming the code is in a file named 'main.py'\n",
      "\n",
      "class TestLambdaHandler(unittest.TestCase):\n",
      "\n",
      "    def test_with_parameters(self):\n",
      "        event = {\n",
      "            'queryStringParameters': {\n",
      "                'first_name': 'John',\n",
      "                'last_name': 'Doe'\n",
      "            }\n",
      "        }\n",
      "        response = lambda_handler(event, None)\n",
      "        self.assertEqual(response['statusCode'], 200)\n",
      "        self.assertEqual(response['body'], json.loads('Hello John Doe!'))\n",
      "\n",
      "    def test_without_parameters(self):\n",
      "        event = {}\n",
      "        response = lambda_handler(event, None)\n",
      "        self.assertEqual(response['statusCode'], 200)\n",
      "        self.assertEqual(response['body'], json.loads('Who are you?'))\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "This Python script uses the `unittest` library to create a test case for the `lambda_handler` function. It includes two test methods: `test_with_parameters` and `test_without_parameters`. Each method creates a test event dictionary and calls the `lambda_handler` function with this event. It then checks the response status code and body to ensure they match the expected values.\n",
      "\n",
      "Please note that the `main.py` is assumed to be the file where the `lambda_handler` function is defined. If the file name is different, please replace `main` with the correct file name.\n",
      "\n",
      "Also, the `json.loads()` function is used to convert the JSON string in the response body to a Python object for comparison. This is necessary because the `assertEqual()` method expects Python objects, not JSON strings.\n"
     ]
    }
   ],
   "source": [
    "code1=\"\"\"\n",
    "Use Python's \"unittest\" library to generate unit tests for the following code:\n",
    "\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    if 'queryStringParameters' in event:    # If parameters\n",
    "        print(event['queryStringParameters']['first_name'])\n",
    "        print(event['queryStringParameters']['last_name'])\n",
    "        body = 'Hello {} {}!'.format(event['queryStringParameters']['first_name'], \n",
    "                                    event['queryStringParameters']['last_name'])  \n",
    "    else:    # If no parameters\n",
    "        print('No parameters!')\n",
    "        body = 'Who are you?'\n",
    "        \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(body)\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code1)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Here are some steps you can use to try running the generated test code:\n",
    "\n",
    "1. Save the `lambda_handler` code in the prompt to a Python file. Include the import statements. Let's assume you name this file `lambda_handler.py`.\n",
    "2. Save the generated test code to a file, for example `test_lambda_handler.py`, in the same directory.\n",
    "\n",
    "You will most likely need to modify the input statement for importing `lambda_handler` that was generated for the test code. For example, if you followed our example naming convention and both files are in the same directory, then the import statement will be:\n",
    "\n",
    "```python\n",
    "from lambda_handler import lambda_handler\n",
    "```\n",
    "\n",
    "Now you can run the tests using the following shell command in the same directory with the files:\n",
    "\n",
    "```shell\n",
    "python -m unittest\n",
    "```\n",
    "\n",
    "Do the tests pass? How good are the tests themselves? Can you modify the prompt with suggestions for improving the quality of the tests. For example, what \"corner cases\" should the tests cover?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "#### Second Example: Generate Tests for Multiple Functions\n",
    "\n",
    "Try running the output tests the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "code2=\"\"\"\n",
    "Use Python's \"unittest\" library to generate unit tests for the following code:\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def load_data(fname):\n",
    "    points = np.loadtxt(fname, delimiter=',') \n",
    "    y_ = points[:,1]\n",
    "    # append '1' to account for the intercept\n",
    "    x_ = np.ones([len(y_),2]) \n",
    "    x_[:,0] = points[:,0]\n",
    "    # display plot\n",
    "    #plt.plot(x_[:,0], y_, 'ro')\n",
    "    #plt.xlabel('x-axis')\n",
    "    #plt.ylabel('y-axis')\n",
    "    #plt.show()\n",
    "    print('data loaded. x:{} y:{}'.format(x_.shape, y_.shape))\n",
    "    return x_, y_\n",
    "\n",
    "def evaluate_cost(x_,y_,params):\n",
    "    tempcost = 0\n",
    "    for i in range(len(y_)):\n",
    "        tempcost += (y_[i] - ((params[0] * x_[i,0]) + params[1])) ** 2 \n",
    "    return tempcost / float(10000)   \n",
    "\n",
    "def evaluate_gradient(x_,y_,params):\n",
    "    m_gradient = 0\n",
    "    b_gradient = 0\n",
    "    N = float(len(y_))\n",
    "    for i in range(len(y_)):\n",
    "        m_gradient += -(2/N) * (x_[i,0] * (y_[i] - ((params[0] * x_[i,0]) + params[1])))\n",
    "        b_gradient += -(2/N) * (y_[i] - ((params[0] * x_[i,0]) + params[1]))\n",
    "    return [m_gradient,b_gradient]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code2)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "#### Third Example: Generate Tests for \"Middleware\" Code\n",
    "\n",
    "We'll also explicit ask for calls to other components to be replaced with \"mocks\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "code3=\"\"\"\n",
    "Use the \"pytest\" library to generate unit tests for the following code. Use mocks and test data for the calls to Kafka:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create Spark session\n",
    "    spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\") # Ignore INFO DEBUG output\n",
    "    df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", topic_name) \\\n",
    "        .load()\n",
    "\n",
    "    df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "    df = df.withColumn(\"data\", from_json(df.value, Sentiment.get_schema())).select(\"data.*\")\n",
    "    df = df \\\n",
    "        .withColumn(\"ts\", to_timestamp(from_unixtime(expr(\"timestamp_ms/1000\")))) \\\n",
    "        .withWatermark(\"ts\", \"1 seconds\") # old data will be removed\n",
    "\n",
    "    # Preprocess the data\n",
    "    df = Sentiment.preprocessing(df)\n",
    "\n",
    "    # text classification to define polarity and subjectivity\n",
    "    df = Sentiment.text_classification(df)\n",
    "\n",
    "    assert type(df) == pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "    row_df = df.select(\n",
    "        to_json(struct(\"id\")).alias('key'),\n",
    "        to_json(struct('text', 'lang', 'ts', 'polarity_v', 'polarity', 'subjectivity_v')).alias(\"value\")\n",
    "    )\n",
    " \n",
    "\n",
    "    # Writing to Kafka\n",
    "    query = row_df\\\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "        .writeStream\\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"topic\", output_topic) \\\n",
    "        .option(\"checkpointLocation\", \"file:/Users/user/tmp\") \\\n",
    "        .start()\n",
    " \n",
    "    query.awaitTermination()\"\"\"\n",
    " \n",
    "replicate_response = granite_via_replicate.invoke(code3)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Fourth Example: More Use of \"Test Doubles\"\n",
    "\n",
    "Here is a scenario with the user input code contains a class definition, which is used to generate test code using test doubles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "code4=\"\"\"\n",
    "Use the \"pytest\" library to generate unit tests for the following class definition. Use test doubles and test data as appropriate to test this class:\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "class Sentiment:\n",
    "    def get_schema():\n",
    "        schema = StructType([\n",
    "            StructField(\"created_at\", StringType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"text\", StringType()),\n",
    "            StructField(\"source\", StringType()),\n",
    "            StructField(\"truncated\", StringType()),\n",
    "            StructField(\"in_reply_to_status_id\", StringType()),\n",
    "            StructField(\"in_reply_to_user_id\", StringType()),\n",
    "            StructField(\"in_reply_to_screen_name\", StringType()),\n",
    "            StructField(\"user\", StringType()),\n",
    "            StructField(\"coordinates\", StringType()),\n",
    "            StructField(\"place\", StringType()),\n",
    "            StructField(\"quoted_status_id\", StringType()),\n",
    "            StructField(\"is_quote_status\", StringType()),\n",
    "            StructField(\"quoted_status\", StringType()),\n",
    "            StructField(\"retweeted_status\", StringType()),\n",
    "            StructField(\"quote_count\", StringType()),\n",
    "            StructField(\"reply_count\", StringType()),\n",
    "            StructField(\"retweet_count\", StringType()),\n",
    "            StructField(\"favorite_count\", StringType()),\n",
    "            StructField(\"entities\", StringType()),\n",
    "            StructField(\"extended_entities\", StringType()),\n",
    "            StructField(\"favorited\", StringType()),\n",
    "            StructField(\"retweeted\", StringType()),\n",
    "            StructField(\"possibly_sensitive\", StringType()),\n",
    "            StructField(\"filter_level\", StringType()),\n",
    "            StructField(\"lang\", StringType()),\n",
    "            StructField(\"matching_rules\", StringType()),\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"timestamp_ms\", StringType())\n",
    "        ])\n",
    "        return schema\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocessing(df):\n",
    "        # words = df.select(explode(split(df.text, \" \")).alias(\"word\"))\n",
    "        df = df.filter(col('text').isNotNull())\n",
    "        df = df.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', r'[^\\x00-\\x7F]+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', r'[\\n\\r]', ' '))\n",
    "        df = df.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', '#', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', ':', ''))\n",
    "        df = df.withColumn('source', regexp_replace('source', '<a href=\"' , ''))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # text classification\n",
    "    @staticmethod\n",
    "    def polarity_detection(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "    @staticmethod\n",
    "    def subjectivity_detection(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "    @staticmethod\n",
    "    def text_classification(words):\n",
    "        # polarity detection\n",
    "        polarity_detection_udf = udf(Sentiment.polarity_detection, FloatType())\n",
    "        words = words.withColumn(\"polarity_v\", polarity_detection_udf(\"text\"))\n",
    "        words = words.withColumn(\n",
    "            'polarity',\n",
    "            when(col('polarity_v') > 0, lit('Positive'))\n",
    "            .when(col('polarity_v') == 0, lit('Neutral'))\n",
    "            .otherwise(lit('Negative'))\n",
    "        )\n",
    "        # subjectivity detection\n",
    "        subjectivity_detection_udf = udf(Sentiment.subjectivity_detection, FloatType())\n",
    "        words = words.withColumn(\"subjectivity_v\", subjectivity_detection_udf(\"text\"))\n",
    "        return words\n",
    "\n",
    "\"\"\"\n",
    "replicate_response = granite_via_replicate.invoke(code4)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Fifth Example: Test External API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "code5=\"\"\"\n",
    "Use the \"pytest\" library to generate unit tests for the following code. Use test doubles and test data as appropriate to test this class:\n",
    "\n",
    "import facebook\n",
    "\n",
    "token = 'your token'\n",
    "\n",
    "graph = facebook.GraphAPI(token)\n",
    "profile = graph.get_object(\"me\")\n",
    "friends = graph.get_connections(\"me\", \"friends\")\n",
    "friend_list = [friend['name'] for friend in friends['data']]\n",
    "print friend_list\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code5)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### Sixth Example: Same as the Fifth Example, but Omit Requesting a Specific Test Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "code6=\"\"\"\n",
    "Generate unit tests for the following code. Use test doubles and test data as appropriate to test this class:\n",
    "\n",
    "import facebook\n",
    "\n",
    "token = 'your token'\n",
    "\n",
    "graph = facebook.GraphAPI(token)\n",
    "profile = graph.get_object(\"me\")\n",
    "friends = graph.get_connections(\"me\", \"friends\")\n",
    "friend_list = [friend['name'] for friend in friends['data']]\n",
    "print friend_list\"\"\"\n",
    "\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(code6)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
