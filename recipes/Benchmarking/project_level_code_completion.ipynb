{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking: project level code completion evaluation\n",
    "\n",
    "This notebook demonstrates how to benchmark the granite models using Long Code Arena's [project level code completion](https://huggingface.co/datasets/JetBrains-Research/lca-project-level-code-completion) dataset.\n",
    "\n",
    "Key concepts:\n",
    "1. Dataset preprocessing: We present multiple ways to prepare the completion context from a GitHub repository.\n",
    "2. Tokenization and truncation: We analyze the token count of our input and truncate it to ensure it fits within the model's context window.\n",
    "3. Prompt engineering: We showcase how building the prompt in different ways can influence the prediction.\n",
    "4. Metrics: We present how to evaluate the predictions using exact match and edit similarity.\n",
    "\n",
    "By the end of this notebook, you'll see how to use an existing code dataset to benchmark the performance of a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before we get started, let's make sure you have the following installed:\n",
    "\n",
    "1. Python 3.10 or later (you can use [pyenv](https://github.com/pyenv/pyenv) to install Python)\n",
    "2. [Ollama](https://ollama.com/)\n",
    "3. [Granite code 8b model](https://ollama.com/library/granite-code:8b), which will serve as our LLM for this tutorial.\n",
    "\n",
    "See the [Coding_Assistant_in_VSCode](../Coding_Assistant_in_VSCode/Coding_Assistant_in_VSCode.ipynb) recipe for instructions on setting up Ollama and installing the Granite models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Before we begin, we need to install the required Python packages. We'll be using:\n",
    "\n",
    "- `git+https://github.com/ibm-granite-community/granite-kitchen`: To interact with the Ollama API\n",
    "- `datasets`: To interact with Huggingface datasets (download, process)\n",
    "- `transformers`: For tokenization and working with language models\n",
    "- `evaluate`: For using the exact match metric\n",
    "- `thefuzz`: For computing the edit similarity between 2 strings\n",
    "\n",
    "These packages will be installed using pip, Python's package installer. If you're running this notebook in a fresh environment, make sure you have pip installed and updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/granite-kitchen transformers[torch] datasets evaluate thefuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"granite-code:8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Ollama client\n",
    "\n",
    "For this example, we're setting the temperature to 0 since we want greedy decoding.\n",
    "\n",
    "You can adjust the context window up to 128000 tokens to see the impact of additional code context on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum context size, you can use up to 128000\n",
    "MAX_LENGTH = 4096\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# we're only trying to predict a single line of code, so 100 tokens are enough\n",
    "model = OllamaLLM(model=MODEL_ID, temperature=0, num_predict=100, num_ctx=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Load the model's tokenizer and define a helper function to truncate the model's prompt when it goes over the imposed limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm-granite/granite-8b-code-instruct-128k\", truncation_side=\"left\")\n",
    "\n",
    "def truncate_prompt(prompt: str, max_length: int = MAX_LENGTH) -> str:\n",
    "    tokenized_prompt = tokenizer(prompt, return_tensors=\"pt\", padding=False, truncation=True, add_special_tokens=False, max_length=max_length)\n",
    "    return tokenizer.decode(tokenized_prompt[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Load the [project level code completion](https://huggingface.co/datasets/JetBrains-Research/lca-project-level-code-completion) dataset. Each data point contains the snapshot of a Python repository along with a number of lines that need to be completed. The lines are split into 6 categories:\n",
    "\n",
    "1. infile – a line contains at least one function or class that was declared in the completion file.\n",
    "2. inproject – a line contains at least one function or class that was declared in the repository snapshot files.\n",
    "3. common – a line contains at least one function or class that was classified to be common, e.g., main, get, etc.\n",
    "4. committed – a line contains at least one function or class that was declared in the files that were created in the same commit as the completion file (excluding the completion file).\n",
    "5. non-informative – a line that satisfies at least on of the following criteria: (i) shorter than 5 characters or longer than 150 characters, (ii) a line with print, (iii) a line with import, (iv) a declaration of a function or a class, (v) a comment or contains an inline comment.\n",
    "6. random – all the lines that do not have any category.\n",
    "\n",
    "We're going to use the small context split that has up to 48K characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"JetBrains-Research/lca-project-level-code-completion\"\n",
    "DATASET_NAME = \"small_context\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(path=DATASET_PATH, name=DATASET_NAME, split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context building\n",
    "\n",
    "Define diferent ways of organizing the additional context coming from all the repository's files apart from the one that we're trying to complete:\n",
    "\n",
    " - concatenate the file path and content in the same order defined in the dataset.\n",
    " - concatenate the file path and content starting from the file path that is furthest away from the completion file.\n",
    "\n",
    "These are inspired by the original [Long Code Arena baselines code](https://github.com/JetBrains-Research/lca-baselines/tree/main/project_level_code_completion/composers), check it out if you'd like to see additional ways of organizing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "sep_symbol = 'METASEP\\n'\n",
    "\n",
    "def path_distance(path_from: str, path_to: str) -> int:\n",
    "    \"\"\"\n",
    "    Compute the number of steps needed to go from one file system path to another.\n",
    "    \"\"\"\n",
    "\n",
    "    divided_path_from = os.path.normpath(path_from).split(os.path.sep)\n",
    "    divided_path_to = os.path.normpath(path_to).split(os.path.sep)\n",
    "    common_len = 0\n",
    "    for el1, el2 in zip(divided_path_from, divided_path_to):\n",
    "        if el1 == el2:\n",
    "            common_len += 1\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # -1 to ignore the file itself\n",
    "    return (len(divided_path_from) - 1 - common_len) + (len(divided_path_to) - 1 - common_len)\n",
    "\n",
    "def sort_filepaths(path_from: str, list_of_filepaths: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Sorts the list of file system paths by how close they are to the provided path.\n",
    "    \"\"\"\n",
    "\n",
    "    max_len = max([len(os.path.normpath(path).split(os.path.sep)) for path in list_of_filepaths])\n",
    "    max_len += len(os.path.normpath(path_from).split(os.path.sep))\n",
    "    paths_by_distance = [list() for _ in range(max_len)]\n",
    "\n",
    "    for path_to in list_of_filepaths:\n",
    "        dist = path_distance(path_from, path_to)\n",
    "        paths_by_distance[dist].append(path_to)\n",
    "\n",
    "    return [path for path_group in paths_by_distance for path in path_group]\n",
    "\n",
    "def default_composer(example):\n",
    "    \"\"\"\n",
    "    Construct the prompt context using the file order present in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    filenames, contents = example[\"repo_snapshot\"][\"filename\"], example[\"repo_snapshot\"][\"content\"]\n",
    "    context_dict = {filename: content for filename, content in zip(filenames, contents)}\n",
    "    repo_name = example[\"repo\"]\n",
    "    context = [path + sep_symbol + content for path, content in context_dict.items()]\n",
    "    example[\"context\"] = f\"{repo_name}{sep_symbol}\" + \"\".join(context)\n",
    "\n",
    "    completion_path = example[\"completion_file\"][\"filename\"]\n",
    "    completion_content = example[\"completion_file\"][\"content\"]\n",
    "    completion = [completion_path + sep_symbol + completion_content]\n",
    "    example[\"completion\"] = sep_symbol + \"\".join(completion)\n",
    "\n",
    "    return example\n",
    "\n",
    "def path_distance_composer(example):\n",
    "    \"\"\"\n",
    "    Construct the prompt context by ordering the files based on how close they are to the commetion file.\n",
    "    \"\"\"\n",
    "\n",
    "    filenames, contents = example[\"repo_snapshot\"][\"filename\"], example[\"repo_snapshot\"][\"content\"]\n",
    "    context_dict = {filename: content for filename, content in zip(filenames, contents)}\n",
    "    repo_name = example[\"repo\"]\n",
    "\n",
    "    completion_path = example[\"completion_file\"][\"filename\"]\n",
    "    sorted_paths = sort_filepaths(completion_path, list(context_dict))\n",
    "    context_content = [path + sep_symbol + context_dict[path] for path in sorted_paths[::-1]]\n",
    "\n",
    "    context_content.append(completion_path + sep_symbol)\n",
    "    example[\"context\"] = f\"{repo_name}{sep_symbol}\" + \"\".join(context_content)\n",
    "\n",
    "    example[\"completion\"] = example[\"completion_file\"][\"content\"]\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "\n",
    "Define helper functions for building the prompt and generating the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prefix(line_num: int, code: str) -> str:\n",
    "    lines = code.split('\\n')\n",
    "    return '\\n'.join(lines[:line_num])\n",
    "\n",
    "def get_line(line_num: int, code: str) -> str:\n",
    "    lines = code.split('\\n')\n",
    "    return lines[line_num]\n",
    "\n",
    "def generate(example, use_context: bool = True):\n",
    "    \"\"\"\n",
    "    Generate the prediction. If `use_context` is `False` use only the contents of the file\n",
    "    missing the line of code, else use also the contents of the other files in the repository. \n",
    "    \"\"\"\n",
    "\n",
    "    completions = example[\"completion_lines\"]\n",
    "    completion = example[\"completion\"]\n",
    "    context = example[\"context\"]\n",
    "\n",
    "    generation_results = {}\n",
    "    for completion_type, completion_lines in completions.items():\n",
    "        generation_results[completion_type] = list()\n",
    "        for line_num in completion_lines:\n",
    "            if use_context:\n",
    "                prompt = \"\\n\".join([context] + [get_prefix(line_num, completion)])\n",
    "            else:\n",
    "                prompt = get_prefix(line_num, completion)\n",
    "            prompt = truncate_prompt(prompt, MAX_LENGTH - 100)\n",
    "            ground_truth = get_line(line_num, completion)\n",
    "            prediction = model.invoke(prompt)\n",
    "            prediction = prediction.strip(\"\\n\")\n",
    "            prediction_line = prediction.split(\"\\n\")[0]\n",
    "            generation_results[completion_type].append({\"ground_truth\": ground_truth, \"prediction\": prediction_line})\n",
    "    \n",
    "    example[\"generation_results\"] = generation_results\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do 3 generations to see how the additional context influences the predictions:\n",
    "\n",
    "1. Generate results using only the content of the completion file.\n",
    "2. Generate results with the original file content ordering.\n",
    "3. Generate results with the file path distance ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the number of samples used, use the entire dataset if you want accurate results\n",
    "SAMPLES = 2\n",
    "if len(ds) > SAMPLES:\n",
    "    ds = ds.select(range(SAMPLES))\n",
    "\n",
    "original_order_ds = ds.map(default_composer)\n",
    "path_distance_order_ds = ds.map(path_distance_composer)\n",
    "\n",
    "# generate predictions using only the completion file's content\n",
    "no_context_ds = original_order_ds.map(generate, fn_kwargs={\"use_context\": False})\n",
    "\n",
    "original_order_context_ds = original_order_ds.map(generate, fn_kwargs={\"use_context\": True})\n",
    "path_distance_order_context_ds = path_distance_order_ds.map(generate, fn_kwargs={\"use_context\": True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Define the metrics we'll use to evaluate the predictions:\n",
    "\n",
    "1. Exact match: 1 if the prediction and the ground truth are identical, 0 otherwise.\n",
    "2. Edit similarity: normalized [Levenshtein distance](https://en.wikipedia.org/wiki/Levenshtein_distance) expressed as percentage [0, 100]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from thefuzz import fuzz\n",
    "\n",
    "exact_match = load(\"exact_match\", module_type=\"metric\")\n",
    "    \n",
    "def calculate_exact_match(generation_results):\n",
    "    results = dict()\n",
    "    for sc_name, gen_res in generation_results.items():\n",
    "        if len(gen_res) > 0:\n",
    "            results[sc_name] = exact_match.compute(\n",
    "                references=[item[\"ground_truth\"].strip() for item in gen_res],\n",
    "                predictions=[item[\"prediction\"].strip() for item in gen_res],\n",
    "            )\n",
    "    return results\n",
    "\n",
    "def calculate_edit_similarity(generation_results):\n",
    "    results = dict()\n",
    "    for sc_name, gen_res in generation_results.items():\n",
    "        similarity = 0.\n",
    "        count = 0\n",
    "        for item in gen_res:\n",
    "            similarity += fuzz.ratio(item[\"prediction\"], item[\"ground_truth\"])\n",
    "            count += 1\n",
    "        if count > 0:\n",
    "            results[sc_name] = {'edit_similarity': similarity / count}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the metrics and aggregate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generations(ds):\n",
    "    generations = {\"all\": list()}\n",
    "\n",
    "    def gather_generations(example):\n",
    "        for key, values in example[\"generation_results\"].items():\n",
    "            if key not in generations:\n",
    "                generations[key] = list()\n",
    "            for result in values:\n",
    "                generations[key].append(result)\n",
    "                generations[\"all\"].append(result)\n",
    "\n",
    "    ds.map(gather_generations)\n",
    "\n",
    "    return generations\n",
    "\n",
    "def merge(d: dict, w: dict) -> dict:\n",
    "    res = dict(**d)\n",
    "    for k, v in w.items():\n",
    "        res[k] |= v\n",
    "\n",
    "    return res\n",
    "\n",
    "results = list()\n",
    "\n",
    "for dataset in [no_context_ds, original_order_context_ds, path_distance_order_context_ds]:\n",
    "    generations = get_generations(dataset)\n",
    "    em = calculate_exact_match(generations)\n",
    "    es = calculate_edit_similarity(generations)\n",
    "    metrics = merge(em, es)\n",
    "\n",
    "    flattened_metrics = dict()\n",
    "    for key, values in metrics.items():\n",
    "        flattened_metrics[f\"{key}-em\"] = values[\"exact_match\"]\n",
    "        flattened_metrics[f\"{key}-es\"] = values[\"edit_similarity\"]\n",
    "\n",
    "    results.append(flattened_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    \"Results without additional context\",\n",
    "    \"Results using original order with additional context\",\n",
    "    \"Results using path distance order with additional context\",\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame.from_records(results, index=names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".lca-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
