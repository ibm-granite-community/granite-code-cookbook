{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engage in a conversation around structured data using `Granite-Code-20B.`\n",
    "This notebook showcases querying any dataset, such as social media dataset (Twitter in this case), using natural language to generate SQL queries with the help of LLM (Granite-Code-20B). As we go through this notebook, we will come across two different kinds of prompting techniques incorporated into this demo. One being `zero-shot` prompting, which includes a straightforward prompt, and another one being `few-shot` prompting, where in addition to a prompt, we have given a couple of examples for the model to understand and generate a better SQL query. At the end, we also tested out the `chain of thought` reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Granite `utils` package\n",
    "\n",
    "This package is a thin shim with various functions that are required for notebooks.\n",
    "\n",
    "To see the implementation of its functions, see the [utils repo](https://github.com/ibm-granite-community/utils/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Install all the necessary libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install replicate\n",
    "!pip install langchain-community\n",
    "!pip install sqlite-utils\n",
    "!pip install google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the database file for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code snippet connects to a SQLite database (social_media.db) using the SQLDatabase utility from the langchain_community package and retrieves its schema.\n",
    "\n",
    "**NOTE** - You can recreate the SQLite database (social_media.db) from CSV files `TwitterDataset/Tweet.csv`, `TwitterDataset/Location.csv`, `TwitterDataset/User.csv` by running Notebook `TwitterDataset/TwitterSyntheticDataGen.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.utilities import SQLDatabase\n",
    "\n",
    "# Note: to run in Colab, you need to upload the TwitterDataset//social_media.db file in the repo to the Colab folder first.\n",
    "db = SQLDatabase.from_uri(\"sqlite:///TwitterDataset//social_media.db\", sample_rows_in_table_info=0)\n",
    "\n",
    "def get_schema():\n",
    "    return db.get_table_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the SQL schema that will be wrapped in the prompt that we pass to the LLM to have more context and generate accurate response. The schema contains 3 tables:\n",
    "\n",
    "**location table:** This table stores information about specific locations, including country, state, state code, and city. It is identified by the LocationID column.\n",
    "\n",
    "**tweet table:** This table contains information about individual tweets, such as the time of posting (Weekday, Hour, Day), language (Lang), whether it was a reshare (IsReshare), tweet statistics (e.g., Reach, RetweetCount, Likes), sentiment, and tweet content (Text). It references two foreign keys: LocationID to the location table and UserID to the user table.\n",
    "\n",
    "**user table:** This table holds user information, identified by UserID, and stores the user's gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_schema = get_schema()\n",
    "print(db_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide your API Token\n",
    "\n",
    "Obtain your `REPLICATE_API_TOKEN` at [replicate.com/account/api-tokens](https://replicate.com/account/api-tokens)\n",
    "\n",
    "There are three ways to provide this value to the cells below.  In order of precedence:\n",
    "\n",
    "1. As an environment variable\n",
    "2. As a Google colab secret\n",
    "3. Supplied by the user using `getpass()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose a Model\n",
    "\n",
    "Two Granite Code models are available in the [`ibm-granite`](https://replicate.com/ibm-granite) org at Replicate.\n",
    "\n",
    "The `find_langchain_model` function below imports the `replicate` package.\n",
    "\n",
    "**Model Parameters**:  \n",
    "i) max_length: Limits the response length to 100 tokens.  \n",
    "ii) temperature=0.0: Ensures a deterministic output by minimizing randomness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.langchain_utils import find_langchain_model\n",
    "\n",
    "#model_id = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "model_id = \"ibm-granite/granite-20b-code-instruct-8k\"\n",
    "\n",
    "granite_via_replicate = find_langchain_model(platform=\"Replicate\", model_id=model_id,\n",
    "                            model_kwargs={\"max_length\":100, \"temperature\":0.0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function **zeroshot_prompt** generates a prompt for a language model to create a SQL query based on a natural language question without prior examples (zero-shot prompting).\n",
    "\n",
    "The prompt is designed as follows:\n",
    "\n",
    "It positions the model as an experienced social media analyst with expertise in SQL.  \n",
    "It provides the database schema for context.  \n",
    "It asks the model to return a SQL query based on the user's question, emphasizing that only the SQL query should be returned.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_prompt(db_schema, question):\n",
    "    prompt = f\"\"\"You are an Social Media analyst with 15 years of experience writing complex SQL queries.\n",
    "    Consider the Twitter tables with the following schema:\n",
    "    {db_schema}\n",
    "\n",
    "    Write a SQL query that would answer the user's question; just return the SQL query and nothing else.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    SQL Query:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function `get_answer_using_zeroshot()`:  \n",
    "**granite_via_replicate.invoke(prompt)**: Sends the prompt to the language model to predict the SQL query.     \n",
    "**result = db.run(sql_query)**: Executes the generated SQL query on the connected social_media.db database.    \n",
    "**return sql_query, result**: Returns the generated SQL query and the result of the query execution.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_using_zeroshot(db_schema, question):\n",
    "    prompt = zeroshot_prompt(db_schema, question)\n",
    "    sql_query = granite_via_replicate.invoke(prompt)\n",
    "    result = db.run(sql_query)\n",
    "    \n",
    "    return sql_query, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing queries with the Zero-Shot Prompting technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many tweets are in English?\"\n",
    "sql_query, result = get_answer_using_zeroshot(db_schema, question)\n",
    "print(f\"sql_query : {sql_query}\")\n",
    "print(f\"result : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Please list the texts of all tweets that are reshared.\"\n",
    "sql_query, result = get_answer_using_zeroshot(db_schema, question)\n",
    "print(f\"sql_query : {sql_query}\")\n",
    "print(f\"result : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many reshared tweets are there in Ontario?\"\n",
    "sql_query, result = get_answer_using_zeroshot(db_schema, question)\n",
    "print(f\"sql_query : {sql_query}\")\n",
    "print(f\"result : {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which city has the highest number of tweets?\"\n",
    "sql_query, result = get_answer_using_zeroshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** - For the next question `What is the total number of tweets made by male users on weekdays?`, the LLM couldn't generate the correct SQL query. It generated the wrong condition to filter weekdays: `tweet.Weekday NOT LIKE '%Weekend%'`. This is fixed using the `Few-Shot prompting` in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the total number of tweets made by male users on weekdays?\"\n",
    "sql_query, result = get_answer_using_zeroshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `fewshot_prompt` function generates a prompt designed for a language model using few-shot prompting, which includes several example SQL queries to guide the model in generating a SQL query from a natural language question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fewshot_prompt(db_schema, user_question):\n",
    "    prompt = f\"\"\"\n",
    "    You are a social media data analyst with 15 years of experience writing complex SQL queries. Generate SQL queries by understanding user input.\n",
    "\n",
    "    You will be given the entire database which is being queired: {db_schema}\n",
    "    Your task is to come with the SQL query from the plaintext provided by the user, which when queried on the above database will result in accurate output.\n",
    "    You are only required generate the SQL query. Do not generate the output from that SQL query or any other explanation.\n",
    "    Do not generate any explanation or comments at all. Just give the SQL query you came up with as it is.\n",
    "    Few examples are given below for your reference.\n",
    "\n",
    "    Example 1: What is the total number of tweets made by female users on weekends?\n",
    "    Answer Query 1: SELECT COUNT(*) AS total_tweets FROM tweet T JOIN user U ON T.UserID = U.UserID WHERE U.Gender = 'Female' AND T.Weekday IN ('Saturday', 'Sunday');\n",
    "\n",
    "    Example 2: What is the average sentiment score for tweets made in the United States?\n",
    "    Answer Query 2: SELECT AVG(T.Sentiment) AS avg_sentiment FROM tweet T JOIN location L ON T.LocationID = L.LocationID WHERE L.Country = 'United States';\n",
    "\n",
    "    Example 3: List the top 5 users with the highest total reach across all their tweets.\n",
    "    Answer Query 3: SELECT U.UserID, SUM(T.Reach) AS total_reach FROM tweet T JOIN user U ON T.UserID = U.UserID GROUP BY U.UserID ORDER BY total_reach DESC LIMIT 5;\n",
    "    User: {user_question}\n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_using_fewshot(db_schema, question):\n",
    "    \"\"\"\n",
    "    Retrieves the SQL query and result by using few-shot learning for a given database schema and question.\n",
    "    Args:\n",
    "        db_schema (str): The database schema.\n",
    "        question (str): The question to be answered.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the SQL query and the result of executing the query on the database.\n",
    "    \"\"\"\n",
    "    prompt = fewshot_prompt(db_schema, question)\n",
    "    sql_query = granite_via_replicate.invoke(prompt)\n",
    "    result = db.run(sql_query)\n",
    "    \n",
    "    return sql_query, result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing queries with the Few-Shot Prompting technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is the total number of tweets made by male users on weekdays?\"\n",
    "sql_query, result = get_answer_using_fewshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** - As seen above, the same exact query in zero-shot yielded wrong answer, and here, as we gave some examples to the model, it was able to come up with a correct SQL query for the same user input.  \n",
    "\n",
    "This goes on to show how having couple of examples related to your domain in the prompt helps out the LLM to accurately generate the desired output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which city has the highest number of tweets?\"\n",
    "sql_query, result = get_answer_using_fewshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** - We have executed the same query in the zero-shot prompting. But the query generated in a few-shot prompting adapted the style of the given examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Find the total reach and average likes for tweets that were reshared by male users.\"\n",
    "sql_query, result = get_answer_using_fewshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Retrieve the list of users who posted tweets with a sentiment score below 0, grouped by their gender, along with the count of such tweets.\"\n",
    "sql_query, result = get_answer_using_fewshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** - For the next question `Find the user(s) who posted the maximum number of tweets in a single day` the LLM couldn't generate the correct SQL query. It missed to include `per day` in the generated SQL query. This is fixed using the `Chain of Thought Reasoning` in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Find the user(s) who posted the maximum number of tweets in a single day.\"\n",
    "sql_query, result = get_answer_using_fewshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain of Thought Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain of Thought (CoT) prompting is used to improve the accuracy of complex reasoning tasks by guiding the model through a structured sequence of steps.  \n",
    "\n",
    "In this technique, we break down the task into a sequence of logical steps to help the language model understand the process required to generate an accurate SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Find the user(s) who posted the maximum number of tweets in a single day.\n",
    "              First calculate tweets posted by each user on each day.\n",
    "              Second sort the result in descending order of the number of tweets.\n",
    "              Third findout the maximum number of tweets posted by a user in a single day.\n",
    "              Finally select the user(s) who posted the maximum number of tweets in a single day.\"\"\"\n",
    "\n",
    "sql_query, result = get_answer_using_fewshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Among all tweets with a positive sentiment, what is the percentage of all those posted by a male user?\n",
    "              First calculate tweets with a positive sentiment those posted by a male user.\n",
    "              Second calculate all tweets with a positive sentiment.\n",
    "              Finally calculate the percentage of the first result of the second result.\"\"\"\n",
    "\n",
    "sql_query, result = get_answer_using_fewshot(db_schema, question)\n",
    "print(f\"sql_query: {sql_query}\")\n",
    "print(f\"result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".codecb_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
