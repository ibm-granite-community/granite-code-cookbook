{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Generate a Retrieval Augmented Generation (RAG) Application Code using IBM Granite Models</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Notebook goals:</h3>\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "1. Connect to ibm-granite/granite-8b-code-instruct-128k hosted on Replicate, and use it to generate a sample RAG Application code\n",
    "2. Connect to ibm-granite/granite-20b-code-instruct-8k hosted on Replicate, and use it to generate a sample RAG Application code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Prerequisites: </h3>\n",
    "\n",
    "1. Create an account on Replicate. \n",
    "\n",
    "2. Copy the Replicate API Token and paste it to an environment file (.env file) created in the same directory as this notebook.<br> Environment variable: `REPLICATE_API_TOKEN`. <br>\n",
    "For example, your .env file can have the following: <br>\n",
    "export REPLICATE_API_TOKEN=< YOUR_REPLICATE_API_TOKEN >\n",
    "\n",
    "3. Install python packages using below pip command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/granite-kitchen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Selection </h3>\n",
    "\n",
    "In this section, we specify the model ID used to invoke specific IBM Granite Models hosted on Replicate platform. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model : granite-8b-code-instruct-128k</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "# model_id = \"ibm-granite/granite-20b-code-instruct-8k\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code snippets use the ibm-granite/granite-8b-code-instruct-128k model to generate a sample code for RAG Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Model Retrieval </h3>\n",
    "\n",
    "Here, we retrieve the model hosted on Replicate and initialize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate\n",
    "from ibm_granite_community.notebook_utils import get_env_var\n",
    "\n",
    "input_parameters = {      \n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 1, \n",
    "    \"max_tokens\": 4096,\n",
    "    \"temperature\": 0.0, \n",
    "}\n",
    "\n",
    "# Find the model via replicate platform\n",
    "\n",
    "model = Replicate(\n",
    "    model=model_id,\n",
    "    model_kwargs=input_parameters,\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of Parameters\n",
    "\n",
    "    top_k: Samples tokens with the highest probabilities until the specified number of tokens is reached. Integer in the range 1 to 100. Default Value = 50. Higher values lead to greater variability. \n",
    "\n",
    "    temperature: Flattens or sharpens the probability distribution over the tokens to be sampled. Floating-point number in the range 0.0 (same as greedy decoding) to 2.0 (maximum creativity). Default Value = 0.7. Higher values lead to greater variability.\n",
    "    \n",
    "    top_p: Samples tokens with the highest probability scores until the sum of the scores reaches the specified threshold value. Floating-point number in the range 0.0 to 1.0. Default Value = 1.0.\n",
    "    \n",
    "    max_tokens: Control the length of the generated response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `zeroshot_prompt` generates a prompt to create a sample RAG Code based on a natural language question without prior examples (zero-shot prompting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeroshot_prompt(question):\n",
    "    prompt = f\"\"\"You are are a Generative AI expert with 20 years of experience writing complex RAG Code. Your task is to write good quality Generative AI code and nothing else. \n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `get_answer_using_zeroshot` generates the result from ibm-granite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_using_zeroshot(question):\n",
    "    prompt = zeroshot_prompt(question)\n",
    "    return model.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h3>Testing with multiple prompts</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question = \"Explain Retrieval Augmented Generation Technique with a sample code.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a complete application code for Retrieval Augmented Generation Technique.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model : granite-20b-code-instruct-8k</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "model_id = \"ibm-granite/granite-20b-code-instruct-8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the model via replicate platform\n",
    "\n",
    "model = Replicate(\n",
    "    model=model_id,\n",
    "    model_kwargs=input_parameters,\n",
    "    replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below code snippets use the ibm-granite/granite-20b-code-instruct-8k model to generate a sample code for RAG Technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Explain Retrieval Augmented Generation Technique with a sample code.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a complete application code for Retrieval Augmented Generation Technique.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Write a complete application code for Retrieval Augmented Generation Technique. Use a Vector Database to store a sample corpus. Finally, use a code model to retrieve correct answers.\"\n",
    "print(f\"result : {get_answer_using_zeroshot(question)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
