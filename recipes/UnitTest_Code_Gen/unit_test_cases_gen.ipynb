{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e98b2a4-cf47-4523-bd07-827683728d31",
   "metadata": {},
   "source": [
    "# Use Remote Granite Code Models (20B) with LangChain for Unit Test Code Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a45854-f221-466b-a0b4-78effeae9f41",
   "metadata": {},
   "source": [
    "## Introduction and Setup\n",
    " \n",
    "This recipe demonstrates how to generate unit tests for Python functions and classes using inference calls against a model hosted remotely on [Replicate](https://replicate.com/). This recipe targets developers who are looking to streamline the process of creating unit tests with minimal manual effort. The user inputs Python code and returns unit test code, incorporating \"test doubles\" for external dependencies.  The notebook depends on Granite [`Utils`](https://github.com/ibm-granite-community/utils) package for integration with LLMs using the [Langchain](https://www.langchain.com/) framework.\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "To run this notebook, ensure you have the following:\n",
    "\n",
    "1. Python version: 3.9 or higher\n",
    "2. A Replicate API token. See the `../recipes/Getting_Started_with_Granite_Code.ipynb` for details.\n",
    "\n",
    "### Model Details:\n",
    "\n",
    "1. Model Platform : Replicate\n",
    "2. Model : IBM Granite 20b Code Instruct 8k\n",
    "3. Model Version : ibm-granite/granite-20b-code-instruct-8k:409a0c68b74df416c7ae2a3f1552101123356f5a2c6e46d681629b62904c605b\n",
    "\n",
    "### Program \n",
    "\n",
    "1. Input: Python code/snippets with instructions for test packages that need to utilized and optional type of unit test case scenarios to be covered.\n",
    "2. Output: Python code with unit test packages and libraries, test doubles, assert implementation for Unit testing of given input.\n",
    "\n",
    "### Disclaimer\n",
    "\n",
    "Results using the 20b code instruct Granite model, with an 8k context, are generally better than the outputs when using the 8b code instruct Granite model, with an 128k context. In either case, the code generated may require additional modifications, depending on the test libraries requested and other aspects of the user input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caa29bb",
   "metadata": {},
   "source": [
    "### Install required Langchain and replicate packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79545db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain_community replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c45de3",
   "metadata": {},
   "source": [
    "### Install Granite `utils` package\n",
    "\n",
    "This package is a thin shim with various functions that are required for notebooks.\n",
    "\n",
    "To see the implementation of its functions, see the [utils repo](https://github.com/ibm-granite-community/utils/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f7fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7432e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import set_env_var, get_env_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f2db6c-f3cc-48a1-94c8-d12849446a77",
   "metadata": {},
   "source": [
    "### Define a Prompt\n",
    "\n",
    "The cells below demonstrate a remote option and a local option for model inference.\n",
    "\n",
    "Both will perform a blocking call using the following system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02f99de7-ff2e-4ae4-b1f2-1ac4453b4c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt =   \"\"\"Role: Python Code Generator.\n",
    "           User Input: <Python code>, optional Test libraries, output file locations ..\n",
    "           Output: Python code for unit testing success and failure conditions of the given input <python code> leveraging the test libraries \n",
    "           Validity: Generates error-free unit test code for the input <python code> by importing those libraries\n",
    "           Test Libraries: User provided test libraries.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab1f48d-a625-4210-80ef-a505c728b331",
   "metadata": {},
   "source": [
    "## Remote Model using Replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386acab-a8c6-41cc-b9b8-e20494ca4828",
   "metadata": {},
   "source": [
    "### Establish Replicate Account\n",
    "\n",
    "To use this remote option, create an account at [Replicate](https://replicate.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ad8255",
   "metadata": {},
   "source": [
    "### Provide your API token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1f99fa",
   "metadata": {},
   "source": [
    "\n",
    "Obtain your REPLICATE_API_TOKEN at replicate.com/account/api-tokens\n",
    "\n",
    "There are three ways to provide this value to the cells below. In order of precedence:\n",
    "\n",
    "1. As an environment variable\n",
    "2. As a Google colab secret\n",
    "3. Supplied by the user using getpass()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db3e69f-82cb-424d-8358-d981453aab93",
   "metadata": {},
   "source": [
    "### Choose a Model\n",
    "\n",
    "Two Granite Code models are available in the [`ibm-granite`](https://replicate.com/ibm-granite) org at Replicate.\n",
    "\n",
    "The `find_langchain_model` function below imports the `replicate` package.\n",
    "\n",
    "Model Arguments are defined using input parameters dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e1569c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_id = \"ibm-granite/granite-8b-code-instruct-128k\"\n",
    "\n",
    "model_id=\"ibm-granite/granite-20b-code-instruct-8k\"\n",
    " \n",
    "input_parameters = {      \n",
    "        \"top_k\": 60,\n",
    "        \"top_p\": 0.3, \n",
    "        \"max_tokens\": 1000,\n",
    "        \"min_tokens\": 0,\n",
    "        \"temperature\": 0.3, \n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"system_prompt\": prompt\n",
    "        }\n",
    "from langchain_community.llms import Replicate\n",
    "\n",
    "granite_via_replicate = Replicate(\n",
    "            model=model_id,\n",
    "            model_kwargs=input_parameters,\n",
    "            replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bcc704-3cab-4da5-8ada-9032d5bafdb4",
   "metadata": {},
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf998562",
   "metadata": {},
   "source": [
    "#### Below use case covers generation of unit test code for the input code leveraging  unittest library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529170fc",
   "metadata": {},
   "source": [
    "#### Invoke the model to generate test cases for application code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=\"\"\" Use unittest library to generate the unit test cases for the below\n",
    "import json\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    if 'queryStringParameters' in event:    # If parameters\n",
    "        print(event['queryStringParameters']['first_name'])\n",
    "        print(event['queryStringParameters']['last_name'])\n",
    "        body = 'Hello {} {}!'.format(event['queryStringParameters']['first_name'], \n",
    "                                    event['queryStringParameters']['last_name'])  \n",
    "    else:    # If no parameters\n",
    "        print('No parameters!')\n",
    "        body = 'Who are you?'\n",
    "        \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps(body)\n",
    "    }\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(codes)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a9ab73",
   "metadata": {},
   "source": [
    "#### Here is another example how user can provide multiple functions code as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f89013",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=\"\"\"Here are the function definitions to be unit tested\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def load_data(fname):\n",
    "    points = np.loadtxt(fname, delimiter=',') \n",
    "    y_ = points[:,1]\n",
    "    # append '1' to account for the intercept\n",
    "    x_ = np.ones([len(y_),2]) \n",
    "    x_[:,0] = points[:,0]\n",
    "    # display plot\n",
    "    #plt.plot(x_[:,0], y_, 'ro')\n",
    "    #plt.xlabel('x-axis')\n",
    "    #plt.ylabel('y-axis')\n",
    "    #plt.show()\n",
    "    print('data loaded. x:{} y:{}'.format(x_.shape, y_.shape))\n",
    "    return x_, y_\n",
    "\n",
    "def evaluate_cost(x_,y_,params):\n",
    "    tempcost = 0\n",
    "    for i in range(len(y_)):\n",
    "        tempcost += (y_[i] - ((params[0] * x_[i,0]) + params[1])) ** 2 \n",
    "    return tempcost / float(10000)   \n",
    "\n",
    "def evaluate_gradient(x_,y_,params):\n",
    "    m_gradient = 0\n",
    "    b_gradient = 0\n",
    "    N = float(len(y_))\n",
    "    for i in range(len(y_)):\n",
    "        m_gradient += -(2/N) * (x_[i,0] * (y_[i] - ((params[0] * x_[i,0]) + params[1])))\n",
    "        b_gradient += -(2/N) * (y_[i] - ((params[0] * x_[i,0]) + params[1]))\n",
    "    return [m_gradient,b_gradient]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(codes)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275593b1",
   "metadata": {},
   "source": [
    "#### Example for user input leveraging middleware code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001fe1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=\"\"\" For the below code write unit test code using pytest library package. Use mocker to mock the kafka calls.\n",
    "if __name__ == \"__main__\":\n",
    "    # create Spark session\n",
    "    spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n",
    "    spark.sparkContext.setLogLevel(\"ERROR\") # Ignore INFO DEBUG output\n",
    "    df = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", topic_name) \\\n",
    "        .load()\n",
    "\n",
    "    df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "\n",
    "    df = df.withColumn(\"data\", from_json(df.value, Sentiment.get_schema())).select(\"data.*\")\n",
    "    df = df \\\n",
    "        .withColumn(\"ts\", to_timestamp(from_unixtime(expr(\"timestamp_ms/1000\")))) \\\n",
    "        .withWatermark(\"ts\", \"1 seconds\") # old data will be removed\n",
    "\n",
    "    # Preprocess the data\n",
    "    df = Sentiment.preprocessing(df)\n",
    "\n",
    "    # text classification to define polarity and subjectivity\n",
    "    df = Sentiment.text_classification(df)\n",
    "\n",
    "    assert type(df) == pyspark.sql.dataframe.DataFrame\n",
    "\n",
    "    row_df = df.select(\n",
    "        to_json(struct(\"id\")).alias('key'),\n",
    "        to_json(struct('text', 'lang', 'ts', 'polarity_v', 'polarity', 'subjectivity_v')).alias(\"value\")\n",
    "    )\n",
    " \n",
    "\n",
    "    # Writing to Kafka\n",
    "    query = row_df\\\n",
    "        .selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "        .writeStream\\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"topic\", output_topic) \\\n",
    "        .option(\"checkpointLocation\", \"file:/Users/user/tmp\") \\\n",
    "        .start()\n",
    " \n",
    "    query.awaitTermination()\"\"\"\n",
    "\n",
    " \n",
    "replicate_response = granite_via_replicate.invoke(codes)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9f926",
   "metadata": {},
   "source": [
    "Here are some steps you can use to try running the generated test code:\n",
    "\n",
    "1. Save the `lambda_handler` code in the prompt to a file. Include the import statements. Let's assume you name this file `lambda_handler.py`.\n",
    "2. Save the generated test code to a file, for example `test_lambda_handler.py`, in the same directory.\n",
    "\n",
    "You will most likely need to modify the input statement for importing `lambda_handler` that was generated for the test code in `test_lambda_hander.py`. For example, if you followed our example naming convention and both files are in the same directory, then the import statement will be:\n",
    "\n",
    "```python\n",
    "from lambda_handler import lambda_handler\n",
    "```\n",
    "\n",
    "Now you can run the tests using the following shell command in the same directory with the files:\n",
    "\n",
    "```shell\n",
    "python -m unittest\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4c4e5f",
   "metadata": {},
   "source": [
    "#### Here is a scenario with user input code as class definition to generate test cases code using test doubles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07f2080",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=\"\"\"Below is the class definition to be tested using pytest library and fixtures. Use below class instances for unit test modules\n",
    "\n",
    "class Sentiment:\n",
    "    def get_schema():\n",
    "        schema = StructType([\n",
    "            StructField(\"created_at\", StringType()),\n",
    "            StructField(\"id\", StringType()),\n",
    "            StructField(\"text\", StringType()),\n",
    "            StructField(\"source\", StringType()),\n",
    "            StructField(\"truncated\", StringType()),\n",
    "            StructField(\"in_reply_to_status_id\", StringType()),\n",
    "            StructField(\"in_reply_to_user_id\", StringType()),\n",
    "            StructField(\"in_reply_to_screen_name\", StringType()),\n",
    "            StructField(\"user\", StringType()),\n",
    "            StructField(\"coordinates\", StringType()),\n",
    "            StructField(\"place\", StringType()),\n",
    "            StructField(\"quoted_status_id\", StringType()),\n",
    "            StructField(\"is_quote_status\", StringType()),\n",
    "            StructField(\"quoted_status\", StringType()),\n",
    "            StructField(\"retweeted_status\", StringType()),\n",
    "            StructField(\"quote_count\", StringType()),\n",
    "            StructField(\"reply_count\", StringType()),\n",
    "            StructField(\"retweet_count\", StringType()),\n",
    "            StructField(\"favorite_count\", StringType()),\n",
    "            StructField(\"entities\", StringType()),\n",
    "            StructField(\"extended_entities\", StringType()),\n",
    "            StructField(\"favorited\", StringType()),\n",
    "            StructField(\"retweeted\", StringType()),\n",
    "            StructField(\"possibly_sensitive\", StringType()),\n",
    "            StructField(\"filter_level\", StringType()),\n",
    "            StructField(\"lang\", StringType()),\n",
    "            StructField(\"matching_rules\", StringType()),\n",
    "            StructField(\"name\", StringType()),\n",
    "            StructField(\"timestamp_ms\", StringType())\n",
    "        ])\n",
    "        return schema\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocessing(df):\n",
    "        # words = df.select(explode(split(df.text, \" \")).alias(\"word\"))\n",
    "        df = df.filter(col('text').isNotNull())\n",
    "        df = df.withColumn('text', regexp_replace('text', r'http\\S+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', r'[^\\x00-\\x7F]+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', r'[\\n\\r]', ' '))\n",
    "        df = df.withColumn('text', regexp_replace('text', '@\\w+', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', '#', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', 'RT', ''))\n",
    "        df = df.withColumn('text', regexp_replace('text', ':', ''))\n",
    "        df = df.withColumn('source', regexp_replace('source', '<a href=\"' , ''))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # text classification\n",
    "    @staticmethod\n",
    "    def polarity_detection(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "\n",
    "    @staticmethod\n",
    "    def subjectivity_detection(text):\n",
    "        return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "    @staticmethod\n",
    "    def text_classification(words):\n",
    "        # polarity detection\n",
    "        polarity_detection_udf = udf(Sentiment.polarity_detection, FloatType())\n",
    "        words = words.withColumn(\"polarity_v\", polarity_detection_udf(\"text\"))\n",
    "        words = words.withColumn(\n",
    "            'polarity',\n",
    "            when(col('polarity_v') > 0, lit('Positive'))\n",
    "            .when(col('polarity_v') == 0, lit('Neutral'))\n",
    "            .otherwise(lit('Negative'))\n",
    "        )\n",
    "        # subjectivity detection\n",
    "        subjectivity_detection_udf = udf(Sentiment.subjectivity_detection, FloatType())\n",
    "        words = words.withColumn(\"subjectivity_v\", subjectivity_detection_udf(\"text\"))\n",
    "        return words\n",
    "\n",
    "\"\"\"\n",
    "replicate_response = granite_via_replicate.invoke(codes)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9597bd",
   "metadata": {},
   "source": [
    "#### Below example showcases code input with external api function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ea939f",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=\"\"\" Given the below code, provide unit testing code \n",
    "\n",
    "import facebook\n",
    "\n",
    "token = 'your token'\n",
    "\n",
    "graph = facebook.GraphAPI(token)\n",
    "profile = graph.get_object(\"me\")\n",
    "friends = graph.get_connections(\"me\", \"friends\")\n",
    "friend_list = [friend['name'] for friend in friends['data']]\n",
    "print friend_list\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(codes)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769367c",
   "metadata": {},
   "source": [
    "#### Another example with external api function call code input but with explicit instruction to use pytest library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30fd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes=\"\"\" Given the below code, provide unit testing code with pytest\n",
    "\n",
    "import facebook\n",
    "\n",
    "token = 'your token'\n",
    "\n",
    "graph = facebook.GraphAPI(token)\n",
    "profile = graph.get_object(\"me\")\n",
    "friends = graph.get_connections(\"me\", \"friends\")\n",
    "friend_list = [friend['name'] for friend in friends['data']]\n",
    "print friend_list\"\"\"\n",
    "\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(codes)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyexe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
