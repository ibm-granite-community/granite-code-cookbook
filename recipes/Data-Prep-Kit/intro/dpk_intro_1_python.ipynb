{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
      "metadata": {
        "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866"
      },
      "source": [
        "# Data Prep Kit Demo 1 - Python version\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/dpk_intro_1_python.ipynb)\n",
        "\n",
        "This notebook will introduce DPK and showcase some of it's capabilities.\n",
        "\n",
        "Here is the workflow\n",
        "\n",
        "![](https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/media/data-prep-kit-3-workflow.png)\n",
        "\n",
        "References\n",
        "- [Data prep kit](https://ibm.github.io/data-prep-kit)  is an open source framework that helps with data wrangling.\n",
        "- [github repo](https://github.com/IBM/data-prep-kit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15976e3",
      "metadata": {
        "id": "b15976e3"
      },
      "source": [
        "## How to run this notebook\n",
        "\n",
        "Two options:\n",
        "\n",
        "- **Option 1 - Google Colab:** easiest option.  no setup required.  Click this link to open this on google colab.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/dpk_intro_1_python.ipynb)\n",
        "- **Option 2 - Local python dev environment:**  Setup using this guide: [setting up python dev environment](setup-python-dev-env.md)\n",
        "\n",
        "The notebook will work as in both environments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb8b0d5c",
      "metadata": {
        "id": "eb8b0d5c"
      },
      "source": [
        "## Step-1: Inspect the Data\n",
        "\n",
        "We will use simple PDFs about Solar system.  The files are [here](https://github.com/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/data/solar-system)\n",
        "\n",
        "- [earth.pdf](https://github.com/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/data/solar-system/earth.pdf)\n",
        "- [mars.pdf](https://github.com/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/data/solar-system/mars.pdf)\n",
        "\n",
        "These PDF files are created from markdown files using the following command\n",
        "\n",
        "```bash\n",
        "pandoc  earth.md  -o earth.pdf\n",
        "pandoc  mars.md  -o mars.pdf\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a0ab6e",
      "metadata": {
        "id": "39a0ab6e"
      },
      "source": [
        "## Step-2: Figure out Runtime Environment\n",
        "\n",
        "### 2.1 - Determine runtime\n",
        "\n",
        "Determine if we are running on Google colab or local python environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe354b7",
      "metadata": {
        "id": "1fe354b7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   print(\"Running in Colab\")\n",
        "   RUNNING_IN_COLAB = True\n",
        "else:\n",
        "   print(\"NOT in Colab\")\n",
        "   RUNNING_IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7c104b",
      "metadata": {
        "id": "8e7c104b"
      },
      "source": [
        "### 2.2 -Download Data if running on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3309799e",
      "metadata": {
        "id": "3309799e"
      },
      "outputs": [],
      "source": [
        "if RUNNING_IN_COLAB:\n",
        "    !mkdir -p 'data/solar-system'\n",
        "    !wget -O 'data/solar-system/earth.pdf'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/data/solar-system/earth.pdf'\n",
        "    !wget -O 'data/solar-system/mars.pdf'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/data/solar-system/mars.pdf'\n",
        "    !wget -O 'utils.py'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/utils.py'\n",
        "    !wget -O 'requirements.txt'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/requirements.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5dc2b68",
      "metadata": {
        "id": "a5dc2b68"
      },
      "source": [
        "### 2.3 - Install dependencies if running on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "1fcec577",
      "metadata": {
        "id": "1fcec577"
      },
      "outputs": [],
      "source": [
        "if RUNNING_IN_COLAB:\n",
        "    ! pip install  --default-timeout=100  -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "243322b8",
      "metadata": {
        "id": "243322b8"
      },
      "source": [
        "### 2.4 - Restart Runtime on COLAB\n",
        "\n",
        "After installing dependencies, be sure <font color=\"red\">restart runtime</font>, so libraries will be loaded\n",
        "\n",
        "You do this by going to **`Runtime --> Restart Session`**\n",
        "\n",
        "Then you can continue to the next step (no need to re-run the notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b10be1",
      "metadata": {
        "id": "e8b10be1"
      },
      "source": [
        "## Step-2: Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "356c66f7",
      "metadata": {
        "id": "356c66f7"
      },
      "source": [
        "### 2.1 - Basic Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4YMZrBuFycl",
      "metadata": {
        "id": "e4YMZrBuFycl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   print(\"Running in Colab\")\n",
        "   RUNNING_IN_COLAB = True\n",
        "else:\n",
        "   print(\"NOT in Colab\")\n",
        "   RUNNING_IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "33345487",
      "metadata": {
        "id": "33345487"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "## Configuration\n",
        "class MyConfig:\n",
        "    pass\n",
        "\n",
        "MY_CONFIG = MyConfig ()\n",
        "\n",
        "MY_CONFIG.INPUT_DATA_DIR = 'data/solar-system'\n",
        "\n",
        "MY_CONFIG.OUTPUT_FOLDER = \"output\"\n",
        "MY_CONFIG.OUTPUT_FOLDER_FINAL = os.path.join(MY_CONFIG.OUTPUT_FOLDER , \"output_final\")\n",
        "\n",
        "## Embedding model\n",
        "MY_CONFIG.EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
      "metadata": {
        "id": "72510ae6-48b0-4b88-9e13-a623281c3a63"
      },
      "source": [
        "### 2.2 - Setup input/outpur directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ac8bee-0960-4309-b225-d7a211b14262",
      "metadata": {
        "id": "60ac8bee-0960-4309-b225-d7a211b14262"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import shutil\n",
        "\n",
        "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
        "    raise Exception (f\"❌ Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
        "\n",
        "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
        "output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_chunk_out')\n",
        "output_docid_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_docid_out')\n",
        "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_exact_dedupe_out')\n",
        "output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '05_embeddings_out')\n",
        "\n",
        "## clear output folder\n",
        "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
        "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print (\"✅ Cleared output directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
      "metadata": {
        "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb"
      },
      "source": [
        "## Step-3: pdf2parquet -  Convert data from PDF to Parquet\n",
        "\n",
        "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
        "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
      "metadata": {
        "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a"
      },
      "source": [
        "### 3.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482605b2-d814-456d-9195-49a2ec454ef0",
      "metadata": {
        "id": "482605b2-d814-456d-9195-49a2ec454ef0"
      },
      "outputs": [],
      "source": [
        "STAGE = 1\n",
        "\n",
        "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
        "output_folder =  output_parquet_dir\n",
        "\n",
        "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
      "metadata": {
        "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b"
      },
      "source": [
        "### 3.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
      "metadata": {
        "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "import ast\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from pdf2parquet_transform import (\n",
        "    pdf2parquet_contents_type_cli_param,\n",
        "    pdf2parquet_contents_types,\n",
        ")\n",
        "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
        "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
        "\n",
        "from data_processing.utils import GB, ParamsUtils\n",
        "\n",
        "\n",
        "# create parameters\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "ingest_config = {\n",
        "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
        "}\n",
        "\n",
        "params = {\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
        "}\n",
        "\n",
        "\n",
        "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
        "# create launcher\n",
        "launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
        "# launch\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"❌ Job failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca790e0",
      "metadata": {
        "id": "5ca790e0"
      },
      "source": [
        "### 3.3 - Inspect Generated output\n",
        "\n",
        "Here we should see one entry per input file processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe59563d",
      "metadata": {
        "id": "fe59563d"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(5)\n",
        "\n",
        "## To display certain columns\n",
        "#parquet_df[['column1', 'column2', 'column3']].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5058a21",
      "metadata": {
        "id": "e5058a21"
      },
      "source": [
        "\n",
        "### 3.4 - Understand the output\n",
        "\n",
        "Here are some interesting attributes to note:\n",
        "\n",
        "- **filename** : original filename\n",
        "- **contents** : text\n",
        "- **document_id**: unique id (UUID) assignd to this document\n",
        "- **hash** : hash of document\n",
        "- **pdf_convert_time** : time to convert this pdf in seconds\n",
        "\n",
        "Let's inspect the **contents** column.  See how the text is being divided up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f870e624",
      "metadata": {
        "id": "f870e624"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "import json\n",
        "\n",
        "pprint.pprint (json.loads(output_df.iloc[0, ]['contents']))\n",
        "# json.loads(output_df.iloc[0, ]['contents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a10c2d",
      "metadata": {
        "id": "e1a10c2d"
      },
      "outputs": [],
      "source": [
        "pprint.pprint (json.loads(output_df.iloc[1, ]['contents']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72274586",
      "metadata": {
        "id": "72274586"
      },
      "source": [
        "##  Step-4: Doc chunks\n",
        "\n",
        "In the previous step, we have extracted text from oru PDFs.  But we have the content of entire file as 'one row' in our parquet output.\n",
        "\n",
        "In this step, we are going to split the documents in chunks, according to their layout segmentation.\n",
        "\n",
        "This transform uses [Quackling](https://github.com/DS4SD/quackling) `HierarchicalChunker`\n",
        "to chunk according to the document layout segmentation, i.e. respecting the original document components as paragraphs, tables, enumerations, etc.\n",
        "It relies on documents converted with the Docling library in the [pdf2parquet transform](https://github.com/IBM/data-prep-kit/blob/dev/transforms/language/pdf2parquet/python/README.md) using the option `contents_type: \"application/json\"`,\n",
        "which provides the required JSON structure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96198fa6",
      "metadata": {
        "id": "96198fa6"
      },
      "source": [
        "### 4.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305f00a3",
      "metadata": {
        "id": "305f00a3"
      },
      "outputs": [],
      "source": [
        "STAGE = 2\n",
        "\n",
        "input_folder = output_parquet_dir # previous output folder is the input folder for the current stage\n",
        "output_folder =  output_chunk_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "369f2cd1",
      "metadata": {
        "id": "369f2cd1"
      },
      "source": [
        "### 4.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7b18d5",
      "metadata": {
        "id": "5b7b18d5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
        "from doc_chunk_transform_python import DocChunkPythonTransformConfiguration\n",
        "\n",
        "\n",
        "# Prepare the commandline params\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "params = {\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # doc_chunk arguments\n",
        "    # ...\n",
        "}\n",
        "\n",
        "# Pass the commandline params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# create launcher\n",
        "launcher = PythonTransformLauncher(DocChunkPythonTransformConfiguration())\n",
        "# launch\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"❌ Job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "213afdf6",
      "metadata": {
        "id": "213afdf6"
      },
      "source": [
        "### 4.3 - Inspect Generated output\n",
        "\n",
        "We would see documents are split into many chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8138d43",
      "metadata": {
        "id": "d8138d43"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
        "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9ca75c",
      "metadata": {
        "id": "9e9ca75c"
      },
      "source": [
        "### 4.4 - Understanding the Output\n",
        "\n",
        "Here we see 2 PDF files are split into 6 chunks.  Basically we see the documents are being split along 'natural boundaris' - paragraphs and bullet points\n",
        "\n",
        "See how **document_id** is carried throughout.  This helps us identify original documents.\n",
        "\n",
        "Also note **contents** is now plain text (not JSON as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3090c950",
      "metadata": {
        "id": "3090c950"
      },
      "outputs": [],
      "source": [
        "output_df[['filename', 'contents']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5f151ae",
      "metadata": {
        "id": "d5f151ae"
      },
      "outputs": [],
      "source": [
        "for f in output_df['filename'].unique():\n",
        "    print ('==========' , f, '===========')\n",
        "    chunks = output_df[output_df['filename'] == f]['contents']\n",
        "    for idx , chunk in enumerate(chunks):\n",
        "        print (f'-------Chunk {idx}------\\n{chunk}\\n-------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ad1c60d",
      "metadata": {
        "id": "7ad1c60d"
      },
      "source": [
        "## Step-5:  DOC ID generation of Chunks\n",
        "\n",
        "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
        "\n",
        " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set **hash_column** to the name of the column, where you want to store it.\n",
        " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set **int_id_column** to the name of the column, where you want to store it.\n",
        "\n",
        "**This is a pre-requisite for fuzzy dedup** in the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1afaa0fd",
      "metadata": {
        "id": "1afaa0fd"
      },
      "source": [
        "### 5.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ffd6f54",
      "metadata": {
        "id": "6ffd6f54"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Input for this stage is the output of exact dedeup component\n",
        "# output of this component makes it possible for fdedup component to run on data.\n",
        "\n",
        "STAGE  = 3\n",
        "\n",
        "input_folder = output_chunk_dir\n",
        "output_folder =  output_docid_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78a51b7",
      "metadata": {
        "id": "f78a51b7"
      },
      "source": [
        "### 5.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc77557",
      "metadata": {
        "id": "5fc77557"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
        "from doc_id_transform_python import DocIDPythonTransformRuntimeConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "params = {\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # orchestrator\n",
        "    # doc id configuration\n",
        "    \"doc_id_doc_column\": \"contents\",\n",
        "    \"doc_id_hash_column\": \"chunk_hash\",\n",
        "    \"doc_id_int_column\": \"chunk_id\",\n",
        "}\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# launch\n",
        "\n",
        "launcher = PythonTransformLauncher(DocIDPythonTransformRuntimeConfiguration())\n",
        "\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"❌ Job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a8c1fa",
      "metadata": {
        "id": "a9a8c1fa"
      },
      "source": [
        "### 5.3 - Inspect Generated output\n",
        "\n",
        "You will notice we have two extra columns\n",
        "\n",
        "- **hash_column**\n",
        "- **int_id_column**\n",
        "\n",
        "But still the same number or rows as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da9adede",
      "metadata": {
        "id": "da9adede"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53",
      "metadata": {
        "id": "4692975c-49ff-41ae-810e-0f5bc0bbdc53"
      },
      "source": [
        "## Step-6: Exact Dedup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
      "metadata": {
        "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe"
      },
      "source": [
        "### 6.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7a1b94",
      "metadata": {
        "id": "4c7a1b94"
      },
      "outputs": [],
      "source": [
        "STAGE  = 4\n",
        "\n",
        "input_folder = output_docid_dir # previous output folder is the input folder for the current stage\n",
        "output_folder =  output_exact_dedupe_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
      "metadata": {
        "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e"
      },
      "source": [
        "### 6.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
      "metadata": {
        "id": "a624b2b2-faad-4325-ac7d-53a840f564ef"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
        "from ededup_transform_python import EdedupPythonTransformRuntimeConfiguration\n",
        "\n",
        "\n",
        "# Prepare the commandline params\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "params = {\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # ededup parameters\n",
        "    \"ededup_doc_column\": \"contents\",\n",
        "    \"ededup_doc_id_column\": \"chunk_hash\",\n",
        "}\n",
        "\n",
        "# Pass the commandline params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# create launcher\n",
        "launcher = PythonTransformLauncher(EdedupPythonTransformRuntimeConfiguration())\n",
        "# launch\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"❌ Job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf1c3c3",
      "metadata": {
        "id": "eaf1c3c3"
      },
      "source": [
        "### 6.3 - Inspect Generated output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d824ebf6",
      "metadata": {
        "id": "d824ebf6"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
        "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
        "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82cc9bb0",
      "metadata": {
        "id": "82cc9bb0"
      },
      "outputs": [],
      "source": [
        "output_df[['filename', 'contents']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc61dffa",
      "metadata": {
        "id": "cc61dffa"
      },
      "outputs": [],
      "source": [
        "for f in output_df['filename'].unique():\n",
        "    print ('==========' , f, '===========')\n",
        "    chunks = output_df[output_df['filename'] == f]['contents']\n",
        "    for idx , chunk in enumerate(chunks):\n",
        "        print (f'-------Chunk {idx}------\\n{chunk}\\n-------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383f40ba",
      "metadata": {
        "id": "383f40ba"
      },
      "source": [
        "### 6.4 - Understanding the output\n",
        "\n",
        "Remember we had 8 chunks initially.  Now we have 7!  One duplicate chunk is removed.\n",
        "\n",
        "If you look at the PDF, the following common paragraph in `earth.pdf` and `mars.pdf`  is removed from one of the documents!  Pretty neat, eh!\n",
        "\n",
        "```text\n",
        "## Solar System\n",
        "\n",
        "Our solar system is a vast and fascinating expanse, comprising eight planets, five dwarf planets, numerous moons, asteroids, comets, and other celestial bodies. At its center lies the star we call the Sun.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85309751-8556-41c6-ac32-84acc941bc8d",
      "metadata": {
        "id": "85309751-8556-41c6-ac32-84acc941bc8d"
      },
      "source": [
        "  ## Fuzzy Dedup\n",
        "\n",
        "And fuzzy dedupe is only available in RAY version.  See this notebook [dpk_intro_1_ray.ipynb](dpk_intro_1_ray.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
      "metadata": {
        "id": "5370950a-2a3a-4143-8218-f9b4808099ba"
      },
      "source": [
        "## Step-7:   Text encoding\n",
        "\n",
        "Encode text for the vector storage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85aba685",
      "metadata": {
        "id": "85aba685"
      },
      "source": [
        "### 7.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
      "metadata": {
        "id": "20a153fa-fd56-401e-86be-4f7617affcc8"
      },
      "outputs": [],
      "source": [
        "STAGE  = 6\n",
        "\n",
        "input_folder = output_exact_dedupe_dir # previous output folder is the input folder for the current stage\n",
        "output_folder =  output_embeddings_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"🏃🏼 STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c97545f4",
      "metadata": {
        "id": "c97545f4"
      },
      "source": [
        "### 7.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228df6b2-bc62-494b-9697-03ece98d7853",
      "metadata": {
        "id": "228df6b2-bc62-494b-9697-03ece98d7853"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from data_processing.runtime.pure_python import PythonTransformLauncher\n",
        "from text_encoder_local_python import TextEncoderPythonTransformConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "params = {\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # text_encoder\n",
        "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
        "}\n",
        "\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "# create launcher\n",
        "launcher = PythonTransformLauncher(TextEncoderPythonTransformConfiguration())\n",
        "\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"✅ Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"❌ Job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b734852c",
      "metadata": {
        "id": "b734852c"
      },
      "source": [
        "### 7.3 - Inspect Generated output\n",
        "\n",
        "You will see a column called `embeddings` added at the end.  This the text content converted into vectors or embeddings.  We used the model `sentence-transformers/all-MiniLM-L6-v2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1c1d09",
      "metadata": {
        "id": "7b1c1d09"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e12630-be6b-4188-a925-77117155617b",
      "metadata": {
        "id": "f5e12630-be6b-4188-a925-77117155617b"
      },
      "source": [
        "## Step-8: Copy output to final output dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
      "metadata": {
        "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
        "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
        "\n",
        "print (f\"✅ Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
