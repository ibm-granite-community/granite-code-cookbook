{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866",
      "metadata": {
        "id": "841e533d-ebb3-406d-9da7-b19e2c5f5866"
      },
      "source": [
        "# Data Prep Kit Demo 1 - Ray Version\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-intro/recipes/Data-Prep-Kit/intro/dpk_intro_1_ray.ipynb)\n",
        "\n",
        "\n",
        "This notebook will introduce DPK and showcase some of it's capabilities.\n",
        "\n",
        "Here is the workflow\n",
        "\n",
        "![](https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-intro/recipes/Data-Prep-Kit/intro/media/data-prep-kit-3-workflow.png)\n",
        "\n",
        "References\n",
        "- [Data prep kit](https://ibm.github.io/data-prep-kit)  is an open source framework that helps with data wrangling.\n",
        "- [github repo](https://github.com/IBM/data-prep-kit)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b15976e3",
      "metadata": {
        "id": "b15976e3"
      },
      "source": [
        "## How to run this notebook\n",
        "\n",
        "Two options:\n",
        "\n",
        "- **Option 1 - Google Colab:** easiest option.  no setup required.  Click this link to open this on google colab.  [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-intro/recipes/Data-Prep-Kit/intro/dpk_intro_1_ray.ipynb)\n",
        "- **Option 2 - Local python dev environment:**  Setup using this guide: [setting up python dev environment](setup-python-dev-env.md)\n",
        "\n",
        "The notebook will work as in both environments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb8b0d5c",
      "metadata": {
        "id": "eb8b0d5c"
      },
      "source": [
        "## Step-1: Inspect the Data\n",
        "\n",
        "We will use simple PDFs about Solar system.  The files are [here](https://github.com/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-intro/recipes/Data-Prep-Kit/data/solar-system)\n",
        "\n",
        "- [earth.pdf](https://github.com/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-intro/recipes/Data-Prep-Kit/data/solar-system/earth.pdf)\n",
        "- [mars.pdf](https://github.com/sujee/granite-code-cookbook/blob/recipe/data-prep-kit-intro/recipes/Data-Prep-Kit/data/solar-system/mars.pdf)\n",
        "\n",
        "These PDF files are created from markdown files using the following command\n",
        "\n",
        "```bash\n",
        "pandoc  earth.md  -o earth.pdf\n",
        "pandoc  mars.md  -o mars.pdf\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a0ab6e",
      "metadata": {
        "id": "39a0ab6e"
      },
      "source": [
        "## Step-2: Figure out Runtime Environment\n",
        "\n",
        "### 2.1 - Determine runtime\n",
        "\n",
        "Determine if we are running on Google colab or local python environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe354b7",
      "metadata": {
        "id": "1fe354b7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   print(\"Running in Colab\")\n",
        "   RUNNING_IN_COLAB = True\n",
        "else:\n",
        "   print(\"NOT in Colab\")\n",
        "   RUNNING_IN_COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e7c104b",
      "metadata": {
        "id": "8e7c104b"
      },
      "source": [
        "### 2.2 -Download Data if running on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3309799e",
      "metadata": {
        "id": "3309799e"
      },
      "outputs": [],
      "source": [
        "if RUNNING_IN_COLAB:\n",
        "    !mkdir -p 'data/solar-system'\n",
        "    !wget -O 'data/solar-system/earth.pdf'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/data/solar-system/earth.pdf'\n",
        "    !wget -O 'data/solar-system/mars.pdf'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/data/solar-system/mars.pdf'\n",
        "    !wget -O 'utils.py'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/utils.py'\n",
        "    !wget -O 'requirements.txt'  'https://raw.githubusercontent.com/sujee/granite-code-cookbook/recipe/data-prep-kit-1-intro/recipes/Data-Prep-Kit/intro/requirements.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5dc2b68",
      "metadata": {
        "id": "a5dc2b68"
      },
      "source": [
        "### 2.3 - Install dependencies if running on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fcec577",
      "metadata": {
        "id": "1fcec577"
      },
      "outputs": [],
      "source": [
        "if RUNNING_IN_COLAB:\n",
        "    ! pip install  --default-timeout=100  -r requirements.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "243322b8",
      "metadata": {
        "id": "243322b8"
      },
      "source": [
        "### 2.4 - Restart Runtime in COLAB\n",
        "\n",
        "After installing dependencies, be sure <font color=\"red\">restart runtime</font>, so libraries will be loaded\n",
        "\n",
        "You do this by going to **`Runtime --> Restart Session`**\n",
        "\n",
        "Then you can continue to the next step (no need to re-run the notebook)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8b10be1",
      "metadata": {
        "id": "e8b10be1"
      },
      "source": [
        "## Step-2: Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "356c66f7",
      "metadata": {
        "id": "356c66f7"
      },
      "source": [
        "### 2.1 - Basic Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4YMZrBuFycl",
      "metadata": {
        "id": "e4YMZrBuFycl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
        "   print(\"Running in Colab\")\n",
        "   RUNNING_IN_COLAB = True\n",
        "else:\n",
        "   print(\"NOT in Colab\")\n",
        "   RUNNING_IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33345487",
      "metadata": {
        "id": "33345487"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "## Configuration\n",
        "class MyConfig:\n",
        "    pass\n",
        "\n",
        "MY_CONFIG = MyConfig ()\n",
        "\n",
        "MY_CONFIG.INPUT_DATA_DIR = 'data/solar-system'\n",
        "MY_CONFIG.OUTPUT_FOLDER = \"output\"\n",
        "MY_CONFIG.OUTPUT_FOLDER_FINAL = os.path.join(MY_CONFIG.OUTPUT_FOLDER , \"output_final\")\n",
        "\n",
        "## Embedding model\n",
        "MY_CONFIG.EMBEDDING_MODEL = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "## RAY CONFIGURATION\n",
        "### For local runs, we can use more parallelism\n",
        "### For google colab, be conservative\n",
        "\n",
        "if RUNNING_IN_COLAB:\n",
        "  MY_CONFIG.RAY_RUNTIME_WORKERS = 2\n",
        "  MY_CONFIG.RAY_NUM_CPUS =  0.3\n",
        "  MY_CONFIG.RAY_MEMORY_GB = 2  # GB\n",
        "else:  # local run\n",
        "  num_cpus_available =  os.cpu_count()\n",
        "  # print (num_cpus_available)\n",
        "  MY_CONFIG.RAY_NUM_CPUS =  1\n",
        "  MY_CONFIG.RAY_MEMORY_GB = 2  # GB\n",
        "  # MY_CONFIG.RAY_RUNTIME_WORKERS = num_cpus_available // 3\n",
        "  MY_CONFIG.RAY_RUNTIME_WORKERS = 2\n",
        "\n",
        "print ('MY_CONFIG.RAY_RUNTIME_WORKERS:', MY_CONFIG.RAY_RUNTIME_WORKERS)\n",
        "print ('MY_CONFIG.RAY_NUM_CPUS:', MY_CONFIG.RAY_NUM_CPUS)\n",
        "print ('MY_CONFIG.RAY_MEMORY_GB:', MY_CONFIG.RAY_MEMORY_GB)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b15e6827",
      "metadata": {
        "id": "b15e6827"
      },
      "outputs": [],
      "source": [
        "## Add parent dir to path\n",
        "import os,sys\n",
        "\n",
        "this_dir = os.path.abspath('')\n",
        "parent_dir = os.path.dirname(this_dir)\n",
        "sys.path.append (os.path.abspath (parent_dir))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72510ae6-48b0-4b88-9e13-a623281c3a63",
      "metadata": {
        "id": "72510ae6-48b0-4b88-9e13-a623281c3a63"
      },
      "source": [
        "### 2.2 - Setup input/outpur directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ac8bee-0960-4309-b225-d7a211b14262",
      "metadata": {
        "id": "60ac8bee-0960-4309-b225-d7a211b14262"
      },
      "outputs": [],
      "source": [
        "import os, sys\n",
        "import shutil\n",
        "\n",
        "if not os.path.exists(MY_CONFIG.INPUT_DATA_DIR ):\n",
        "    raise Exception (f\"âŒ Input folder MY_CONFIG.INPUT_DATA_DIR = '{MY_CONFIG.INPUT_DATA_DIR}' not found\")\n",
        "\n",
        "output_parquet_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '01_parquet_out')\n",
        "output_chunk_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '02_chunk_out')\n",
        "output_docid_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '03_docid_out')\n",
        "output_exact_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '04_exact_dedupe_out')\n",
        "output_fuzzy_dedupe_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '05_fuzzy_dedupe_out')\n",
        "output_embeddings_dir = os.path.join (MY_CONFIG.OUTPUT_FOLDER, '06_embeddings_out')\n",
        "\n",
        "## clear output folder\n",
        "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER, ignore_errors=True)\n",
        "shutil.os.makedirs(MY_CONFIG.OUTPUT_FOLDER, exist_ok=True)\n",
        "\n",
        "print (\"âœ… Cleared output directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb",
      "metadata": {
        "id": "2449e5c7-078c-4ad6-a2f6-21d39d4da3fb"
      },
      "source": [
        "## Step-3: pdf2parquet -  Convert data from PDF to Parquet\n",
        "\n",
        "This step is reading the input folder containing all PDF files and ingest them in a parquet table using the [Docling package](https://github.com/DS4SD/docling).\n",
        "The documents are converted into a JSON format which allows to easily chunk it in the later steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a",
      "metadata": {
        "id": "c0c574c4-9dc4-4dab-9ad6-b5338207e67a"
      },
      "source": [
        "### 3.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482605b2-d814-456d-9195-49a2ec454ef0",
      "metadata": {
        "id": "482605b2-d814-456d-9195-49a2ec454ef0"
      },
      "outputs": [],
      "source": [
        "STAGE = 1\n",
        "\n",
        "input_folder = MY_CONFIG.INPUT_DATA_DIR\n",
        "output_folder =  output_parquet_dir\n",
        "\n",
        "print (f\"ðŸƒðŸ¼ STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b",
      "metadata": {
        "id": "9bb15f02-ab5c-4525-a536-cfa1fd2ba70b"
      },
      "source": [
        "### 3.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26",
      "metadata": {
        "id": "b0cd8ebd-bf71-42d6-a397-8df0c7b66a26"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "import ast\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from pdf2parquet_transform import (\n",
        "    pdf2parquet_contents_type_cli_param,\n",
        "    pdf2parquet_contents_types,\n",
        ")\n",
        "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
        "from pdf2parquet_transform_python import Pdf2ParquetPythonTransformConfiguration\n",
        "from pdf2parquet_transform_ray import Pdf2ParquetRayTransformConfiguration\n",
        "\n",
        "from data_processing.utils import GB, ParamsUtils\n",
        "\n",
        "\n",
        "# create parameters\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS, \"memory\": MY_CONFIG.RAY_MEMORY_GB * GB}\n",
        "ingest_config = {\n",
        "    pdf2parquet_contents_type_cli_param: pdf2parquet_contents_types.JSON,\n",
        "}\n",
        "\n",
        "params = {\n",
        "    # where to run\n",
        "    \"run_locally\": True,\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    \"data_files_to_use\": ast.literal_eval(\"['.pdf']\"),\n",
        "    # orchestrator\n",
        "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
        "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
        "}\n",
        "\n",
        "\n",
        "sys.argv = ParamsUtils.dict_to_req(d=(params | ingest_config))\n",
        "# create launcher\n",
        "launcher = RayTransformLauncher(Pdf2ParquetRayTransformConfiguration())\n",
        "# launcher = PythonTransformLauncher(Pdf2ParquetPythonTransformConfiguration())\n",
        "# launch\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"âœ… Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"âŒ Ray job failed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ca790e0",
      "metadata": {
        "id": "5ca790e0"
      },
      "source": [
        "### 3.3 - Inspect Generated output\n",
        "\n",
        "Here we should see one entry per input file processed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe59563d",
      "metadata": {
        "id": "fe59563d"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Output dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(5)\n",
        "\n",
        "## To display certain columns\n",
        "#parquet_df[['column1', 'column2', 'column3']].head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5058a21",
      "metadata": {
        "id": "e5058a21"
      },
      "source": [
        "\n",
        "### 3.4 - Understand the output\n",
        "\n",
        "Here are some interesting attributes to note:\n",
        "\n",
        "- **filename** : original filename\n",
        "- **contents** : text\n",
        "- **document_id**: unique id (UUID) assignd to this document\n",
        "- **hash** : hash of document\n",
        "- **pdf_convert_time** : time to convert this pdf in seconds\n",
        "\n",
        "Let's inspect the **contents** column.  See how the text is being divided up!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f870e624",
      "metadata": {
        "id": "f870e624"
      },
      "outputs": [],
      "source": [
        "import pprint\n",
        "import json\n",
        "\n",
        "pprint.pprint (json.loads(output_df.iloc[0, ]['contents']))\n",
        "# json.loads(output_df.iloc[0, ]['contents'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1a10c2d",
      "metadata": {
        "id": "e1a10c2d"
      },
      "outputs": [],
      "source": [
        "pprint.pprint (json.loads(output_df.iloc[1, ]['contents']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72274586",
      "metadata": {
        "id": "72274586"
      },
      "source": [
        "##  Step-4: Doc chunks\n",
        "\n",
        "In the previous step, we have extracted text from oru PDFs.  But we have the content of entire file as 'one row' in our parquet output.\n",
        "\n",
        "In this step, we are going to split the documents in chunks, according to their layout segmentation.\n",
        "\n",
        "This transform uses [Quackling](https://github.com/DS4SD/quackling) `HierarchicalChunker`\n",
        "to chunk according to the document layout segmentation, i.e. respecting the original document components as paragraphs, tables, enumerations, etc.\n",
        "It relies on documents converted with the Docling library in the [pdf2parquet transform](https://github.com/IBM/data-prep-kit/blob/dev/transforms/language/pdf2parquet/python/README.md) using the option `contents_type: \"application/json\"`,\n",
        "which provides the required JSON structure."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96198fa6",
      "metadata": {
        "id": "96198fa6"
      },
      "source": [
        "### 4.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "305f00a3",
      "metadata": {
        "id": "305f00a3"
      },
      "outputs": [],
      "source": [
        "STAGE = 2\n",
        "\n",
        "input_folder = output_parquet_dir # previous output folder is the input folder for the current stage\n",
        "output_folder =  output_chunk_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"ðŸƒðŸ¼ STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "369f2cd1",
      "metadata": {
        "id": "369f2cd1"
      },
      "source": [
        "### 4.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7b18d5",
      "metadata": {
        "id": "5b7b18d5"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
        "from doc_chunk_transform_ray import DocChunkRayTransformConfiguration\n",
        "\n",
        "\n",
        "# Prepare the commandline params\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
        "params = {\n",
        "    # where to run\n",
        "    \"run_locally\": True,\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # orchestrator\n",
        "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
        "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
        "    # doc_chunk arguments\n",
        "    # ...\n",
        "}\n",
        "\n",
        "# Pass the commandline params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# create launcher\n",
        "launcher = RayTransformLauncher(DocChunkRayTransformConfiguration())\n",
        "# launch\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"âœ… Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"âŒ Ray job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "213afdf6",
      "metadata": {
        "id": "213afdf6"
      },
      "source": [
        "### 4.3 - Inspect Generated output\n",
        "\n",
        "We would see documents are split into many chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8138d43",
      "metadata": {
        "id": "d8138d43"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (f\"Files processed : {input_df.shape[0]:,}\")\n",
        "print (f\"Chunks created : {output_df.shape[0]:,}\")\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9ca75c",
      "metadata": {
        "id": "9e9ca75c"
      },
      "source": [
        "### 4.4 - Understanding the Output\n",
        "\n",
        "Here we see 2 PDF files are split into 6 chunks.  Basically we see the documents are being split along 'natural boundaris' - paragraphs and bullet points\n",
        "\n",
        "See how **document_id** is carried throughout.  This helps us identify original documents.\n",
        "\n",
        "Also note **contents** is now plain text (not JSON as before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3090c950",
      "metadata": {
        "id": "3090c950"
      },
      "outputs": [],
      "source": [
        "output_df[['filename', 'contents']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5f151ae",
      "metadata": {
        "id": "d5f151ae"
      },
      "outputs": [],
      "source": [
        "for f in output_df['filename'].unique():\n",
        "    print ('==========' , f, '===========')\n",
        "    chunks = output_df[output_df['filename'] == f]['contents']\n",
        "    for idx , chunk in enumerate(chunks):\n",
        "        print (f'-------Chunk {idx}------\\n{chunk}\\n-------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20217298",
      "metadata": {
        "id": "20217298"
      },
      "source": [
        "## Step-5:  DOC ID generation\n",
        "\n",
        "This transform annotates documents with document \"ids\". It supports the following transformations of the original data:\n",
        "\n",
        " - Adding document hash: this enables the addition of a document hash-based id to the data. The hash is calculated with `hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()`. To enable this annotation, set **hash_column** to the name of the column, where you want to store it.\n",
        " - Adding integer document id: this allows the addition of an integer document id to the data that is unique across all rows in all tables provided to the transform() method. To enable this annotation, set **int_id_column** to the name of the column, where you want to store it.\n",
        "\n",
        "**This is a pre-requisite for fuzzy dedup** in the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66811f5b",
      "metadata": {
        "id": "66811f5b"
      },
      "source": [
        "### 5.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f747c0d",
      "metadata": {
        "id": "1f747c0d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Input for this stage is the output of exact dedeup component\n",
        "# output of this component makes it possible for fdedup component to run on data.\n",
        "\n",
        "STAGE  = 3\n",
        "\n",
        "input_folder = output_chunk_dir\n",
        "output_folder =  output_docid_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"ðŸƒðŸ¼ STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18aa0fe1",
      "metadata": {
        "id": "18aa0fe1"
      },
      "source": [
        "### 5.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e9e145",
      "metadata": {
        "id": "f6e9e145"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
        "from doc_id_transform_ray import DocIDRayTransformRuntimeConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
        "params = {\n",
        "    # where to run\n",
        "    \"run_locally\": True,\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # orchestrator\n",
        "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
        "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
        "    # doc id configuration\n",
        "    \"doc_id_doc_column\": \"contents\",\n",
        "    \"doc_id_hash_column\": \"chunk_hash\",\n",
        "    \"doc_id_int_column\": \"chunk_id\",\n",
        "}\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# launch\n",
        "\n",
        "launcher = RayTransformLauncher(DocIDRayTransformRuntimeConfiguration())\n",
        "\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"âœ… Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"âŒ Ray job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4954402f",
      "metadata": {
        "id": "4954402f"
      },
      "source": [
        "### 5.3 - Inspect Generated output\n",
        "\n",
        "You will notice we have two extra columns\n",
        "\n",
        "- **hash_column**\n",
        "- **int_id_column**\n",
        "\n",
        "But still the same number or rows as before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1911179a",
      "metadata": {
        "id": "1911179a"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "852829dc",
      "metadata": {
        "id": "852829dc"
      },
      "source": [
        "## Step-6: Exact Dedup\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe",
      "metadata": {
        "id": "5acfd3a2-a236-4143-bcfc-15804f1da7fe"
      },
      "source": [
        "### 6.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c7a1b94",
      "metadata": {
        "id": "4c7a1b94"
      },
      "outputs": [],
      "source": [
        "STAGE  = 4\n",
        "\n",
        "input_folder = output_docid_dir # previous output folder is the input folder for the current stage\n",
        "output_folder =  output_exact_dedupe_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"ðŸƒðŸ¼ STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e",
      "metadata": {
        "id": "3661cb37-39c7-4b09-a784-925bfa9eaf1e"
      },
      "source": [
        "### 6.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a624b2b2-faad-4325-ac7d-53a840f564ef",
      "metadata": {
        "id": "a624b2b2-faad-4325-ac7d-53a840f564ef"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
        "from ededup_transform_ray import EdedupRayTransformRuntimeConfiguration\n",
        "\n",
        "\n",
        "# Prepare the commandline params\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
        "params = {\n",
        "    # where to run\n",
        "    \"run_locally\": True,\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # orchestrator\n",
        "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
        "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
        "    # ededup parameters\n",
        "    \"ededup_hash_cpu\": 0.5,\n",
        "    \"ededup_num_hashes\": 2,\n",
        "    \"ededup_doc_column\": \"contents\",\n",
        "    \"ededup_doc_id_column\": \"chunk_hash\",\n",
        "}\n",
        "\n",
        "# Pass the commandline params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# create launcher\n",
        "launcher = RayTransformLauncher(EdedupRayTransformRuntimeConfiguration())\n",
        "# launch\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"âœ… Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"âŒ Ray job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf1c3c3",
      "metadata": {
        "id": "eaf1c3c3"
      },
      "source": [
        "### 6.3 - Inspect Generated output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d824ebf6",
      "metadata": {
        "id": "d824ebf6"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "print (f\"Input chunks before exact dedupe : {input_df.shape[0]:,}\")\n",
        "print (f\"Output chunks after exact dedupe : {output_df.shape[0]:,}\")\n",
        "print (\"Duplicate chunks removed :  \", (input_df.shape[0] - output_df.shape[0]))\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82cc9bb0",
      "metadata": {
        "id": "82cc9bb0"
      },
      "outputs": [],
      "source": [
        "output_df[['filename', 'contents']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc61dffa",
      "metadata": {
        "id": "cc61dffa"
      },
      "outputs": [],
      "source": [
        "for f in output_df['filename'].unique():\n",
        "    print ('==========' , f, '===========')\n",
        "    chunks = output_df[output_df['filename'] == f]['contents']\n",
        "    for idx , chunk in enumerate(chunks):\n",
        "        print (f'-------Chunk {idx}------\\n{chunk}\\n-------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "383f40ba",
      "metadata": {
        "id": "383f40ba"
      },
      "source": [
        "### 6.4 - Understanding the output\n",
        "\n",
        "Remember we had 8 chunks initially.  Now we have 7!  One duplicate chunk is removed.\n",
        "\n",
        "If you look at the PDF, the following common paragraph in `earth.pdf` and `mars.pdf`  is removed from one of the documents!  Pretty neat, eh!\n",
        "\n",
        "```text\n",
        "## Solar System\n",
        "\n",
        "Our solar system is a vast and fascinating expanse, comprising eight planets, five dwarf planets, numerous moons, asteroids, comets, and other celestial bodies. At its center lies the star we call the Sun.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85309751-8556-41c6-ac32-84acc941bc8d",
      "metadata": {
        "id": "85309751-8556-41c6-ac32-84acc941bc8d"
      },
      "source": [
        "## Step-7: Fuzzy Dedup\n",
        "\n",
        "Post exact deduplication, fuzzy deduplication is applied with the goal of removing code files that may have **slight variations** and thereby unbiasing\n",
        "the data further.\n",
        "\n",
        "Small variations are quite commonly seen in code data in the form of variations in the values of variables, addittion of logging statements etc."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcf574a3-b287-419c-9c86-07b828b41ca6",
      "metadata": {
        "id": "fcf574a3-b287-419c-9c86-07b828b41ca6"
      },
      "source": [
        "### 7.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399",
      "metadata": {
        "id": "9e431c8c-c7c7-48de-ba5f-2c4649c35399"
      },
      "outputs": [],
      "source": [
        "## Input to this component is the output of doc_id generator component.\n",
        "\n",
        "STAGE  = 5\n",
        "\n",
        "input_folder = output_docid_dir # previous output folder is the input folder for the current stage\n",
        "output_folder =  output_fuzzy_dedupe_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"ðŸƒðŸ¼ STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3",
      "metadata": {
        "id": "f4c82a8f-b513-4fe5-b172-d41b104b54f3"
      },
      "source": [
        "### 7.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f",
      "metadata": {
        "id": "3864ff77-e9a8-48f7-973b-c3b3aef1a94f"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "from data_processing.utils import ParamsUtils\n",
        "from fdedup_transform_ray import FdedupRayTransformConfiguration\n",
        "from data_processing_ray.runtime.ray import RayTransformLauncher\n",
        "\n",
        "# create parameters\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
        "code_location = {\"github\": \"github\", \"commit_hash\": \"12345\", \"path\": \"path\"}\n",
        "params = {\n",
        "    # where to run\n",
        "    \"run_locally\": True,\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # Orchestration parameters\n",
        "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
        "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
        "    # columns used\n",
        "    \"fdedup_doc_column\": \"contents\",\n",
        "    \"fdedup_id_column\": \"chunk_id\",\n",
        "    \"fdedup_cluster_column\": \"chunk_hash\",\n",
        "    # infrastructure\n",
        "    \"fdedup_bucket_cpu\": 0.3,\n",
        "    \"fdedup_doc_cpu\": 0.3,\n",
        "    \"fdedup_mhash_cpu\": 0.3,\n",
        "    \"fdedup_num_doc_actors\": 1,\n",
        "    \"fdedup_num_bucket_actors\": 1,\n",
        "    \"fdedup_num_minhash_actors\": 1,\n",
        "    \"fdedup_num_preprocessors\": 1,\n",
        "    # fuzzy parameters\n",
        "    \"fdedup_num_permutations\": 64,\n",
        "    \"fdedup_threshold\": 0.7, # (default 0.8)\n",
        "    \"fdedup_shingles_size\": 5,\n",
        "    \"fdedup_delimiters\": \" \"\n",
        "}\n",
        "\n",
        "# Pass commandline params\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "\n",
        "# launch\n",
        "\n",
        "launcher = RayTransformLauncher(FdedupRayTransformConfiguration())\n",
        "\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"âœ… Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"âŒ Ray job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f8cd11",
      "metadata": {
        "id": "a6f8cd11"
      },
      "source": [
        "### 7.3 - Inspect Generated output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e899ad60",
      "metadata": {
        "id": "e899ad60"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "print (\"Duplicate chunks removed  by fuzzy-dedupe:  \", (input_df.shape[0] - output_df.shape[0]))\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab7ea52b",
      "metadata": {
        "id": "ab7ea52b"
      },
      "outputs": [],
      "source": [
        "output_df[['filename', 'contents']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bdd3515",
      "metadata": {
        "id": "6bdd3515"
      },
      "outputs": [],
      "source": [
        "for f in output_df['filename'].unique():\n",
        "    print ('==========' , f, '===========')\n",
        "    chunks = output_df[output_df['filename'] == f]['contents']\n",
        "    for idx , chunk in enumerate(chunks):\n",
        "        print (f'-------Chunk {idx}------\\n{chunk}\\n-------')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b34d9c6",
      "metadata": {
        "id": "2b34d9c6"
      },
      "source": [
        "### 7.4- Understanding the output\n",
        "\n",
        "So we started with 7 rows and ended up with 6.  Fuzzy dedupe removed the following **very similar** chunk.\n",
        "\n",
        "These are pretty similar chunks except for the words 'the' and 'our'\n",
        "\n",
        "**earth.pdf**\n",
        "\n",
        "`For more details about *our* Solar system see Chapter 1.`\n",
        "\n",
        "**mars.pdf**\n",
        "\n",
        "`For more details about *the* Solar system see Chapter 1.`\n",
        "\n",
        "Pretty neat, eh? ðŸ‘\n",
        "\n",
        "### Configuring Fuzzy de-dupe\n",
        "\n",
        "You can tweak fuzzy dedupe by tweaking the following parameters\n",
        "\n",
        "```python\n",
        "# fuzzy parameters\n",
        "    \"fdedup_num_permutations\": 64,\n",
        "    \"fdedup_threshold\": 0.7, #  (default 0.8)\n",
        "    \"fdedup_shingles_size\": 5,\n",
        "    \"fdedup_delimiters\": \" \"\n",
        "```\n",
        "\n",
        "In our case, we set `fdedup_threshold` parameter to 0.7.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5370950a-2a3a-4143-8218-f9b4808099ba",
      "metadata": {
        "id": "5370950a-2a3a-4143-8218-f9b4808099ba"
      },
      "source": [
        "## Step-8:   Text encoding\n",
        "\n",
        "Encode text for the vector storage."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85aba685",
      "metadata": {
        "id": "85aba685"
      },
      "source": [
        "### 8.1 - Set Input/output Folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20a153fa-fd56-401e-86be-4f7617affcc8",
      "metadata": {
        "id": "20a153fa-fd56-401e-86be-4f7617affcc8"
      },
      "outputs": [],
      "source": [
        "STAGE  = 6\n",
        "\n",
        "input_folder = output_fuzzy_dedupe_dir # previous output folder is the input folder for the current stage\n",
        "output_folder =  output_embeddings_dir\n",
        "\n",
        "input_df = read_parquet_files_as_df(input_folder)  ## for debug purposes\n",
        "\n",
        "print (f\"ðŸƒðŸ¼ STAGE-{STAGE}: Processing input='{input_folder}' --> output='{output_folder}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c97545f4",
      "metadata": {
        "id": "c97545f4"
      },
      "source": [
        "### 8.2 - Execute"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "228df6b2-bc62-494b-9697-03ece98d7853",
      "metadata": {
        "id": "228df6b2-bc62-494b-9697-03ece98d7853"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "\n",
        "from text_encoder_transform_ray import TextEncoderRayTransformConfiguration\n",
        "\n",
        "local_conf = {\n",
        "    \"input_folder\": input_folder,\n",
        "    \"output_folder\": output_folder,\n",
        "}\n",
        "worker_options = {\"num_cpus\" : MY_CONFIG.RAY_NUM_CPUS}\n",
        "params = {\n",
        "    # where to run\n",
        "    \"run_locally\": True,\n",
        "    # Data access. Only required parameters are specified\n",
        "    \"data_local_config\": ParamsUtils.convert_to_ast(local_conf),\n",
        "    # orchestrator\n",
        "    \"runtime_worker_options\": ParamsUtils.convert_to_ast(worker_options),\n",
        "    \"runtime_num_workers\": MY_CONFIG.RAY_RUNTIME_WORKERS,\n",
        "    # text_encoder\n",
        "    \"text_encoder_model_name\": MY_CONFIG.EMBEDDING_MODEL,\n",
        "}\n",
        "\n",
        "sys.argv = ParamsUtils.dict_to_req(d=params)\n",
        "# create launcher\n",
        "launcher = RayTransformLauncher(TextEncoderRayTransformConfiguration())\n",
        "# Launch the ray actor(s) to process the input\n",
        "\n",
        "return_code = launcher.launch()\n",
        "\n",
        "if return_code == 0:\n",
        "    print (f\"âœ… Stage:{STAGE} completed successfully\")\n",
        "else:\n",
        "    raise Exception (\"âŒ Ray job failed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b734852c",
      "metadata": {
        "id": "b734852c"
      },
      "source": [
        "### 8.3 - Inspect Generated output\n",
        "\n",
        "You will see a column called `embeddings` added at the end.  This the text content converted into vectors or embeddings.  We used the model `sentence-transformers/all-MiniLM-L6-v2`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b1c1d09",
      "metadata": {
        "id": "7b1c1d09"
      },
      "outputs": [],
      "source": [
        "from utils import read_parquet_files_as_df\n",
        "\n",
        "output_df = read_parquet_files_as_df(output_folder)\n",
        "\n",
        "print (\"Input data dimensions (rows x columns)= \", input_df.shape)\n",
        "print (\"Output data dimensions (rows x columns)= \", output_df.shape)\n",
        "\n",
        "output_df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e12630-be6b-4188-a925-77117155617b",
      "metadata": {
        "id": "f5e12630-be6b-4188-a925-77117155617b"
      },
      "source": [
        "## Step-9: Copy output to final output dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207",
      "metadata": {
        "id": "16dee3b8-31dc-4168-8adb-f2a0a0b5e207"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "\n",
        "shutil.rmtree(MY_CONFIG.OUTPUT_FOLDER_FINAL, ignore_errors=True)\n",
        "shutil.copytree(src=output_folder, dst=MY_CONFIG.OUTPUT_FOLDER_FINAL)\n",
        "\n",
        "print (f\"âœ… Copied output from '{output_folder}' --> '{MY_CONFIG.OUTPUT_FOLDER_FINAL}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0a6728",
      "metadata": {
        "id": "dc0a6728"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}