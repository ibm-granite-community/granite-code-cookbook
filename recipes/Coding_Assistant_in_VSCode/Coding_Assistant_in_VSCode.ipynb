{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# Running a Coding Assistant in VSCode using Continue and Granite Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Continue in VSCode\n",
    "\n",
    "Follow the [Continue Quickstart Docs](https://docs.continue.dev/quickstart) to install the Continue extension for VSCode. Once Continue is installed, you should see its icon in the left nav bar in VSCode. Selecting that will bring up the configuration window, and later the chat window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to a local model on Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Install Ollama\n",
    "[Download the Ollama server](https://ollama.com/download) from the Ollama website.\n",
    "\n",
    "\n",
    "Alternatively, install Ollama with homebrew and start the server.\n",
    "\n",
    "`% brew install ollama`\n",
    "\n",
    "`% ollama serve`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Install a Granite Code model on Ollama\n",
    "\n",
    "Granite-code models are available in the [ollama.com library](https://ollama.com/library/granite-code). Install a model with ollama, which will download a manifest and then the model files. \n",
    "\n",
    "```\n",
    "% ollama pull granite-code:3b\n",
    "% ollama pull granite-code:8b\n",
    "% ollama pull granite-code:20b\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Continue to connect to a model on Ollama\n",
    "\n",
    "After setting up Continue, you will need to configure a new model. Navigate to the Continue extension by clicking the icon in the left nav bar. If you are not presented with configuration options, click on the \"+\" button in the top bar of the chat window and selecting \"Add Model\" from the model dropdown.\n",
    "\n",
    "![alt text](images/AddNewModel.png)\n",
    "\n",
    "Select \"Start with a Provider\", then select \"Ollama\" from the provider list.\n",
    "\n",
    "![alt text](images/SelectOllamaProvider.png)\n",
    "\n",
    "The following will be added to your Continue config at `~/.continue/config.json`, which you can view by searching for `> Continue: Open config.json` in VSCode:\n",
    "\n",
    "```\n",
    "{\n",
    "   \"models\": [\n",
    "     {\n",
    "      \"model\": \"AUTODETECT\",\n",
    "      \"title\": \"Ollama\",\n",
    "      \"apiBase\": \"http://localhost:11434\",\n",
    "      \"provider\": \"ollama\"\n",
    "    }\n",
    "...\n",
    "```\n",
    "\n",
    "Now all models you have installed with Ollama will be available in the Continue chat window. \n",
    "Note: After pulling new models with ollama, you may need to restart VSCode to get Continue to autodetect them for the model dropdown in its chat window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to a remote model on Replicate\n",
    "\n",
    "If you prefer to use a remote model rather than a local model on Ollama, you can run models hosted Replicate. \n",
    "\n",
    "Acquire an API key from [Replicate](https://replicate.com/) and connect to Granite Code models hosted there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Continue to connect to a model on Replicate\n",
    "\n",
    "Add following model block to your Continue config at `~/.continue/config.json`, which you can open by searching for `> Continue: Open config.json` in VSCode:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"title\": \"Replicate Granite Code 8b/128k\",\n",
    "      \"provider\": \"replicate\",\n",
    "      \"model\": \"ibm-granite/granite-8b-code-instruct-128k:797c070dc871d8fca417d7d188cf050778d7ce21a0318d26711a54207e9ee698\",\n",
    "      \"apiKey\": \"***\"\n",
    "    },\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Granite Code for Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use the Granite Code model in Continue\n",
    "\n",
    "Select the Granite Code Model from the model dropdown in the Continue chat window:\n",
    "\n",
    "![alt text](images/SelectGraniteModel.png)\n",
    "\n",
    "Now you have a chat window for the Granite Code model in your IDE!\n",
    "\n",
    "![alt text](images/CodeChatOllamaGC20b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Code Generation in this Notebook\n",
    "With this notebook open in VSCode, and your cursor in the code cell below, press `cmd+I` and enter the following text:\n",
    "\n",
    "`Write a function that adds two numbers together and returns the sum, and use that function to solve an example problem.`\n",
    "\n",
    "Granite Code will generate the following code (or similar):\n",
    "\n",
    "```\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add(2, 3)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "You can then press \"Accept\" and run the code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 3 and 5 is 8.\n"
     ]
    }
   ],
   "source": [
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add_numbers(3, 5)\n",
    "print(f\"The sum of 3 and 5 is {result}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Granite Code for Tab Autocompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a model for Tab Autocompletion\n",
    "\n",
    "In the search bar, type `> Continue` and click `Open config.json`.\n",
    "\n",
    "![Edit Continue configuration](images/EditContinueConfig.png)\n",
    "\n",
    "Modify the Tab Autocomplete config entry to look like this:\n",
    "\n",
    "```\n",
    "  \"tabAutocompleteModel\": {\n",
    "    \"title\": \"Ollama Granite Code 3b\",\n",
    "    \"provider\": \"ollama\",\n",
    "    \"model\": \"granite-code:3b\"\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tab Autocompletion\n",
    "\n",
    "In the editor, start typing a function name, or write a comment, and the assistant will propose code. Press `tab` to accept.\n",
    "\n",
    "![Tab Autocomplete example](images/TabAutocompletion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Gabe Goodhart: Build a local AI co-pilot using IBM Granite Code, Ollama, and Continue](https://developer.ibm.com/tutorials/awb-local-ai-copilot-ibm-granite-code-ollama-continue/)\n",
    "* [Install Continue in VSCode - Quickstart](https://https://docs.continue.dev/quickstart)\n",
    "* [Download and run Ollama](https://ollama.com/download)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
