{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "# Running a Coding Assistant in VSCode using Continue and Granite Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Continue in VSCode\n",
    "\n",
    "Follow the [Continue Quickstart Docs](https://docs.continue.dev/quickstart) to install the Continue extension for VSCode. Once Continue is installed, you should see its icon in the left nav bar in VSCode. Selecting that will bring up the configuration window, and later the chat window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to a local model on Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install a Granite Code model on Ollama\n",
    "\n",
    "See the [Getting Started with Ollama](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Ollama.ipynb) recipe for instructions on setting up Ollama and installing the Granite models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Continue to connect to a model on Ollama\n",
    "\n",
    "After setting up Continue, you will need to configure a new model. Navigate to the Continue extension by clicking the icon in the left nav bar. If you are not presented with configuration options, click on the \"+\" button in the top bar of the chat window and selecting \"Add Model\" from the model dropdown.\n",
    "\n",
    "![alt text](images/AddNewModel.png)\n",
    "\n",
    "Select \"Start with a Provider\", then select \"Ollama\" from the provider list.\n",
    "\n",
    "![alt text](images/SelectOllamaProvider.png)\n",
    "\n",
    "The following will be added to your Continue config at `~/.continue/config.json`, which you can view by searching for `> Continue: Open config.json` in VSCode:\n",
    "\n",
    "```\n",
    "{\n",
    "   \"models\": [\n",
    "     {\n",
    "      \"model\": \"AUTODETECT\",\n",
    "      \"title\": \"Ollama\",\n",
    "      \"apiBase\": \"http://localhost:11434\",\n",
    "      \"provider\": \"ollama\"\n",
    "    }\n",
    "...\n",
    "```\n",
    "\n",
    "Now all models you have installed with Ollama will be available in the Continue chat window. \n",
    "Note: After pulling new models with ollama, you may need to restart VSCode to get Continue to autodetect them for the model dropdown in its chat window.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to a remote model on Replicate\n",
    "\n",
    "If you prefer to use a remote model rather than a local model on Ollama, you can run models hosted Replicate. \n",
    "\n",
    "Acquire an API key from [Replicate](https://replicate.com/) to access the Granite Code models hosted there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Continue to connect to a model on Replicate\n",
    "\n",
    "Add following model block to your Continue config at `~/.continue/config.json`, which you can open by searching for `> Continue: Open config.json` in VSCode:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"models\": [\n",
    "    {\n",
    "      \"title\": \"Replicate Granite Code 8b/128k\",\n",
    "      \"provider\": \"replicate\",\n",
    "      \"model\": \"ibm-granite/granite-8b-code-instruct-128k:797c070dc871d8fca417d7d188cf050778d7ce21a0318d26711a54207e9ee698\",\n",
    "      \"apiKey\": \"***\"\n",
    "    },\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Granite Code for Chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use the Granite Code model in Continue\n",
    "\n",
    "Select the Granite Code Model from the model dropdown in the Continue chat window:\n",
    "\n",
    "![alt text](images/SelectGraniteModel.png)\n",
    "\n",
    "Now you have a chat window for the Granite Code model in your IDE!\n",
    "\n",
    "![alt text](images/CodeChatOllamaGC20b.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Code Generation in this Notebook\n",
    "With this notebook open in VSCode, and your cursor in the code cell below, press `cmd+I` and enter the following text:\n",
    "\n",
    "`Write a function that adds two numbers together and returns the sum, and use that function to solve an example problem.`\n",
    "\n",
    "Granite Code will generate the following code (or similar):\n",
    "\n",
    "```\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add(2, 3)\n",
    "print(result)\n",
    "```\n",
    "\n",
    "You can then press \"Accept\" and run the code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 3 and 5 is 8.\n"
     ]
    }
   ],
   "source": [
    "def add_numbers(a, b):\n",
    "    return a + b\n",
    "\n",
    "result = add_numbers(3, 5)\n",
    "print(f\"The sum of 3 and 5 is {result}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Granite Code for Tab Autocompletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure a model for Tab Autocompletion\n",
    "\n",
    "In the search bar, type `> Continue` and click `Open config.json`.\n",
    "\n",
    "![Edit Continue configuration](images/EditContinueConfig.png)\n",
    "\n",
    "For speed and accuracy, select a small model with a large context window; here we use Granite Code 3b/128k. Modify the Tab Autocomplete config entry to look like this:\n",
    "\n",
    "```\n",
    "  \"tabAutocompleteModel\": {\n",
    "    \"title\": \"Ollama Granite Code 3b\",\n",
    "    \"provider\": \"ollama\",\n",
    "    \"model\": \"granite-code:3b\"\n",
    "  },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Tab Autocompletion\n",
    "\n",
    "In the editor, start typing a function name, or write a comment, and the assistant will propose code. Press `tab` to accept.\n",
    "\n",
    "![Tab Autocomplete example](images/TabAutocompletion.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Have fun!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "* [Gabe Goodhart: Build a local AI co-pilot using IBM Granite Code, Ollama, and Continue](https://developer.ibm.com/tutorials/awb-local-ai-copilot-ibm-granite-code-ollama-continue/)\n",
    "* [Install Continue in VSCode - Quickstart](https://https://docs.continue.dev/quickstart)\n",
    "* [Download and run Ollama](https://ollama.com/download)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
