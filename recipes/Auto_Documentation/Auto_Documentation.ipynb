{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Auto-generating Documentation: A Long Document Summarization Approach\n",
    "\n",
    "This notebook demonstrates an innovative application of long document summarization techniques to automatically generate documentation for Python code. By treating a codebase as a \"long document,\" we leverage AI-powered language models to comprehend, distill, and explain complex code structures.\n",
    "\n",
    "Key concepts:\n",
    "1. Document preprocessing: We fetch and format code from a GitHub repository, similar to how one might prepare a long text document for summarization.\n",
    "2. Chunking and tokenization: We analyze the token count of our code \"document\" to ensure it fits within the model's context window, a crucial step in long document processing.\n",
    "3. Prompt engineering: We craft a specialized prompt that guides the AI to focus on key aspects of the code, much like how summarization prompts direct models to capture essential information.\n",
    "4. AI-powered analysis: Using the Replicate API, we access a large language model capable of understanding code semantics and generating human-readable explanations.\n",
    "5. Structured output: We instruct the model to produce documentation in a consistent format, analogous to generating structured summaries from lengthy texts.\n",
    "\n",
    "This approach demonstrates how techniques traditionally used for summarizing long articles, reports, or books can be adapted for technical documentation tasks. It showcases the versatility of large language models in processing and synthesizing complex information, whether it's natural language or programming code.\n",
    "\n",
    "By the end of this notebook, you'll see how principles of long document summarization can be applied to streamline and enhance the software documentation process, potentially saving developers significant time and effort."
   ],
   "metadata": {
    "id": "Q6rko_ANX0EC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Dependencies\n",
    "\n",
    "Before we begin, we need to install the required Python packages. We'll be using:\n",
    "\n",
    "- `replicate`: To interact with the Replicate API for accessing AI models\n",
    "- `transformers`: For tokenization and working with language models\n",
    "\n",
    "These packages will be installed using pip, Python's package installer. If you're running this notebook in a fresh environment, make sure you have pip installed and updated (if you are in Colab, this is done for you)."
   ],
   "metadata": {
    "id": "IwS1CzAbaFzq"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zUHQD71qgqf"
   },
   "outputs": [],
   "source": [
    "!pip install replicate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set Replicate Token\n",
    "\n",
    "To use the Replicate API, we need to authenticate our requests. This is done using an API token.\n",
    "\n",
    "For security reasons, it's best to store this token as an environment variable rather than hardcoding it into our script. In this notebook, we're using Google Colab's `userdata` feature to securely store and retrieve the token.  (If you are not using Colab, change this cell to\n",
    "\n",
    "```\n",
    "os.environ['REPLICATE_API_TOKEN'] = \"your-token\"\n",
    "```\n",
    "\n",
    "Remember to never share your API tokens publicly or commit them to version control systems."
   ],
   "metadata": {
    "id": "ydrVWz7EYHh9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "\n",
    "if os.environ.get('REPLICATE_API_TOKEN') is None:\n",
    "    \"\"\"Replicate API token not set, we're probably in Colab. Let's try to fetch it.\"\"\"\n",
    "    from google.colab import userdata\n",
    "    userdata = userdata.get(\"replicate-api-token\")\n",
    "    os.environ['REPLICATE_API_TOKEN'] = userdata.get('REPLICATE_API_TOKEN')"
   ],
   "metadata": {
    "id": "TSkiGBY4qo32"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define a function for downloading a repository\n",
    "\n",
    "We'll create a function to fetch code from a GitHub repository. This allows us to easily obtain the code we want to document.\n",
    "\n",
    "Key points about this function:\n",
    "- It uses the GitHub API to retrieve repository contents\n",
    "- It can handle both files and directories recursively\n",
    "- The function formats the code with appropriate language tags for better display\n",
    "- An optional GitHub token can be provided for increased API rate limits and access to private repositories\n",
    "\n",
    "Note on GitHub tokens:\n",
    "A GitHub token is not required for public repositories, but it can be beneficial. With a token, you can:\n",
    "1. Access private repositories\n",
    "2. Have a higher rate limit for API requests\n",
    "3. Fetch more detailed information about the repository\n",
    "\n",
    "To create a GitHub token, go to your GitHub account settings, select \"Developer settings\", then \"Personal access tokens\".  Find more information [here](https://docs.github.com/en/rest/authentication/authenticating-to-the-rest-api?apiVersion=2022-11-28)."
   ],
   "metadata": {
    "id": "5d0sWaZ7YLHN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from time import sleep\n",
    "\n",
    "def get_github_repo_contents(repo_url, directory_path, github_token = None):\n",
    "    # Extracting the owner and repo name from the URL\n",
    "    repo_parts = repo_url.rstrip('/').split('/')\n",
    "    owner = repo_parts[-2]\n",
    "    repo = repo_parts[-1]\n",
    "\n",
    "    api_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{directory_path}\"\n",
    "    if github_token is not None:\n",
    "      headers = {'Authorization': f'token {github_token}'}\n",
    "      response = requests.get(api_url, headers = headers)\n",
    "    else:\n",
    "      response = requests.get(api_url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    contents = response.json()\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for item in contents:\n",
    "        if item['type'] == 'file':\n",
    "            file_response = requests.get(item['download_url'])\n",
    "            file_response.raise_for_status()\n",
    "            file_content = file_response.text\n",
    "            language = item['name'].split('.')[-1]\n",
    "            if language == 'py':\n",
    "                language = 'python'\n",
    "            elif language == 'js':\n",
    "                language = 'javascript'\n",
    "            result.append(f\"{item['path']}\\n```{language}\\n{file_content}\\n```\")\n",
    "        elif item['type'] == 'dir':\n",
    "            # Recursively go through subdirectories\n",
    "            subdirectory_contents = get_github_repo_contents(repo_url, item['path'], github_token)\n",
    "            result.append(subdirectory_contents)\n",
    "        sleep(0.1)\n",
    "\n",
    "    return \"\\n\\n\".join(result)\n"
   ],
   "metadata": {
    "id": "3JFi40LArpIa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get code from `ibm-granite-community/utils`\n",
    "\n",
    "In this example, we're focusing on the `ibm-granite-community/utils` repository, specifically the `ibm_granite_community` directory. This directory contains various utility functions that we want to document.\n",
    "\n",
    "By specifying this directory, we ensure that we're only fetching the relevant code and not unnecessary files or directories. This helps to keep our input focused and reduces the likelihood of exceeding token limits in our AI model."
   ],
   "metadata": {
    "id": "H-06VQn1YmtU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "prompt = get_github_repo_contents(\"https://github.com/ibm-granite-community/utils\", \"ibm_granite_community\")"
   ],
   "metadata": {
    "id": "k2wS6rGJsu-T"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Count the tokens\n",
    "\n",
    "Before sending our code to the AI model, it's crucial to understand how much of the model's capacity we're using. Language models typically have a limit on the number of tokens they can process in a single request.\n",
    "\n",
    "Key points:\n",
    "- We're using the `granite-8B-Code-instruct-128k` model, which has a context window of 128,000 tokens\n",
    "- The context window includes both the input (our code) and the output (the generated documentation)\n",
    "- Tokenization can vary between models, so we use the specific tokenizer for our chosen model\n",
    "- If our input is too large, we may need to split it into smaller chunks or summarize it\n",
    "\n",
    "Understanding token count helps us optimize our prompts and ensure we're using the model efficiently."
   ],
   "metadata": {
    "id": "HYuQmgRJY0n5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"ibm-granite/granite-8B-Code-instruct-128k\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "print(f\"Your git repo load has {len(tokenizer(prompt, return_tensors='pt')['input_ids'][0])} tokens\")"
   ],
   "metadata": {
    "id": "7JqmvTqbWPgl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create our prompt and call the model in Replicate\n",
    "\n",
    "This is where we construct our final prompt and send it to the AI model for processing.\n",
    "\n",
    "Our approach involves:\n",
    "1. Combining the code we fetched with specific instructions for documentation\n",
    "2. Using a template to guide the model's output format\n",
    "3. Calling the Replicate API with our constructed prompt and additional parameters\n",
    "\n",
    "Key considerations:\n",
    "- The prompt includes both the code and instructions for how to document it\n",
    "- We use a response template to ensure consistent formatting across functions\n",
    "- Parameters like `max_tokens`, `temperature`, and `system_prompt` can be adjusted to fine-tune the model's behavior\n",
    "- The output is streamed, allowing for real-time display of the generated documentation\n",
    "\n",
    "This step is where the magic happens - transforming our code into human-readable documentation."
   ],
   "metadata": {
    "id": "ygNmITWQZAZ8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import replicate\n",
    "\n",
    "full_prompt = prompt + \"\"\"\n",
    "\n",
    "Provide detailed developer documentation for each function provided above.\n",
    "\n",
    "Response Template:\n",
    "## `function_name`\n",
    "\n",
    "* _param1_: (type) description\"\n",
    "\n",
    "Synopsis of the function\n",
    "\n",
    "_**returns**_:\n",
    "\"\"\"\n",
    "\n",
    "output = replicate.run(\n",
    "    \"ibm-granite/granite-8b-code-instruct-128k\",\n",
    "    input={\n",
    "\n",
    "        \"prompt\": full_prompt,\n",
    "        \"max_tokens\": 10000,\n",
    "        \"min_tokens\": 0,\n",
    "        \"temperature\": 0.75,\n",
    "        \"system_prompt\": \"You are a helpful assistant.\",\n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"\".join(output))\n"
   ],
   "metadata": {
    "id": "yu4HeuqWqvOj"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
