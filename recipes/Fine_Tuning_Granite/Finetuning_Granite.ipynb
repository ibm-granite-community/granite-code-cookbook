{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning a Code Instruction Model for Legal Tasks  \n",
        "  \n",
        "In this notebook, we demonstrate how to fine-tune the `ibm-granite/granite-3b-code-instruct-2k` model, a small code instruction model, on a legal dataset using the qLoRA (Quantized Low-Rank Adaptation) technique. This experiment serves two primary purposes:  \n",
        "  \n",
        "1. Educational: It showcases the process of adapting a pre-trained model to a new domain.  \n",
        "2. Practical: It illustrates how a model's interpretation of domain-specific terms (like 'inheritance') can shift based on the training data.  \n",
        "  \n",
        "We'll walk through several key steps:  \n",
        "- Installing necessary dependencies  \n",
        "- Loading and exploring the dataset  \n",
        "- Setting up the quantized model  \n",
        "- Performing a sanity check  \n",
        "- Configuring and executing the training process  \n",
        "  \n",
        "By the end, we'll have a model that has learned to interpret legal concepts, demonstrating the power and flexibility of transfer learning in NLP.  "
      ],
      "metadata": {
        "id": "xIj6q15Z5gpO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8e_ji3fd81d"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets accelerate bitsandbytes peft trl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Preparation  \n",
        "  \n",
        "We're using the `legal-llama-instruction1` dataset, which contains various legal questions and corresponding answers. This dataset is particularly valuable for our purpose as it covers a wide range of legal concepts, providing a solid foundation to evaluate how our model's understanding of legal terms differs from technical definitions.  \n",
        "  \n",
        "The dataset is split into training and testing subsets, allowing us to both train the model and evaluate its performance on unseen data.  "
      ],
      "metadata": {
        "id": "eS-vhW2P6MdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('dliu1/legal-llama-instruction1')\n",
        "\n",
        "split_dataset = dataset['train'].train_test_split(test_size=0.2)\n",
        "dataset_loadtime = timeit.default_timer() - start_time\n"
      ],
      "metadata": {
        "id": "np-FUjTfeLNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading and Quantization  \n",
        "  \n",
        "Next, we load the quantized model. Quantization is a technique that reduces the model size and increases inference speed by approximating the weights of the model. We use the `BitsAndBytes` library, which allows us to load the model in a more memory-efficient format without significantly compromising performance.  \n",
        "  \n",
        "This step is crucial as it enables us to work with a large language model within the memory constraints of our hardware, making the fine-tuning process more accessible and efficient.  "
      ],
      "metadata": {
        "id": "VoQp5knP6UpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timeit.default_timer()\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer\n",
        "\n",
        "model_checkpoint = \"ibm-granite/granite-3b-code-instruct-2k\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16 # if not set will throw a warning about slow speeds when training\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_checkpoint,\n",
        "  quantization_config=bnb_config,\n",
        "  trust_remote_code=True\n",
        ")\n",
        "\n",
        "model_loadtime = timeit.default_timer() - start_time\n"
      ],
      "metadata": {
        "id": "DBCHhukYox8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Sanity Check  \n",
        "  \n",
        "Before proceeding with fine-tuning, we perform a sanity check on the loaded model. We feed it an example prompt about 'inheritance' to ensure it produces intelligible and contextually appropriate responses.  \n",
        "  \n",
        "At this stage, the model should interpret 'inheritance' in a programming context, explaining how classes inherit properties and methods from one another. This output serves as a baseline, allowing us to compare how the model's understanding shifts after fine-tuning on legal data.  \n",
        "\n",
        "Note that the output is truncated because of us setting `max_new_tokens=100`"
      ],
      "metadata": {
        "id": "zfX4ltjB6mgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timeit.default_timer()\n",
        "input_text = \"<|user>What does 'inheritance' mean?\\n<|assistant|>\\n\"\n",
        "# In coding, \"inheritance\" typically refers to a mechanism in object-oriented programming where a new class can inherit the properties and behavior of an existing class.\n",
        "# In a legal context, \"inheritance\" refers to the process by which a person receives property, titles, or debts from a deceased person.\n",
        "\n",
        "\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "model_check_loadtime = timeit.default_timer() - start_time\n"
      ],
      "metadata": {
        "id": "LQzeV3ytV42p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Output\n",
        "\n",
        "```\n",
        "Inheritance is a mechanism by which one class acquires the properties and behaviors of another class. In object-oriented programming, inheritance allows a new class to inherit the properties and methods of an existing class, known as the parent or base class. This can be useful for code reuse and creating a hierarchy of classes.\n",
        "\n",
        "For example, let's say we have a base class called \"Vehicle\" that has properties like \"make\" and \"model\". We can create a subclass called \"Car\" that\n",
        "```"
      ],
      "metadata": {
        "id": "-7P4iUMPy2hF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup  \n",
        "  \n",
        "In this section, we set up the training environment. Key steps include:  \n",
        "  \n",
        "1. Defining the format for training prompts to align with the model's expected inputs.  \n",
        "2. Configuring the qLoRA technique, which allows us to fine-tune the model efficiently by only training a small number of additional parameters.  \n",
        "3. Setting up the `SFTTrainer` (Supervised Fine-Tuning Trainer) with appropriate hyperparameters.  \n",
        "  \n",
        "This setup allows us to enhance specific aspects of the model's performance without retraining the entire model from scratch, saving computational resources and time.  "
      ],
      "metadata": {
        "id": "l0CXDP4h7CfZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timeit.default_timer()\n",
        "def formatting_prompts_func(example):\n",
        "    output_texts = []\n",
        "    for i in range(len(example['question'])):\n",
        "        text = f\"<|system|>\\nYou are a helpful assistant\\n<|user|>\\n{example['question'][i]}\\n<|assistant|>\\n{example['answer'][i]}<|endoftext|>\"\n",
        "        output_texts.append(text)\n",
        "    return output_texts\n",
        "\n",
        "response_template = \"\\n<|assistant|>\\n\"\n",
        "\n",
        "from trl import DataCollatorForCompletionOnlyLM\n",
        "\n",
        "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
        "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "# Apply qLoRA\n",
        "qlora_config = LoraConfig(\n",
        "    r=16,  # The rank of the Low-Rank Adaptation\n",
        "    lora_alpha=32,  # Scaling factor for the adapted layers\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],  # Layer names to apply LoRA to\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the SFTTrainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-4,\n",
        "    per_device_train_batch_size=6,\n",
        "    per_device_eval_batch_size=6,\n",
        "    num_train_epochs=3,\n",
        "    logging_steps=100,\n",
        "    fp16=True\n",
        ")\n",
        "\n",
        "max_seq_length = 250\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=split_dataset['train'],\n",
        "    eval_dataset=split_dataset['test'],\n",
        "    tokenizer=tokenizer,\n",
        "    peft_config = qlora_config,\n",
        "    formatting_func=formatting_prompts_func,\n",
        "    data_collator=collator,\n",
        "    max_seq_length=max_seq_length,\n",
        ")\n",
        "\n",
        "training_setup_loadtime = timeit.default_timer() - start_time\n"
      ],
      "metadata": {
        "id": "Ng0W0ULxsE_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Process  \n",
        "  \n",
        "With all the preparations complete, we now start the training process. The model will be exposed to numerous examples from our legal dataset, gradually adjusting its understanding of legal concepts.  \n",
        "  \n",
        "We'll monitor the training loss over time, which should decrease as the model improves its performance on the task. After training, we'll save the fine-tuned model for future use.  "
      ],
      "metadata": {
        "id": "uKIe2z_THh35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = timeit.default_timer()\n",
        "# Start training\n",
        "trainer.train()\n",
        "training_time = timeit.default_timer() - start_time\n"
      ],
      "metadata": {
        "id": "oZ2rUKO2sdae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the Fine-tuned Model  \n",
        "  \n",
        "After the training process is complete, it's crucial to save our fine-tuned model. This step ensures that we can reuse the model later without having to retrain it. We'll save both the model weights and the tokenizer, as they work in tandem to process and generate text.  \n",
        "  \n",
        "Saving the model allows us to distribute it, use it in different environments, or continue fine-tuning it in the future. It's a critical step in the machine learning workflow, preserving the knowledge our model has acquired through the training process.  "
      ],
      "metadata": {
        "id": "zUjBCOZVIaas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./results\")\n",
        "tokenizer.save_pretrained(\"./results\")"
      ],
      "metadata": {
        "id": "rK1i3CFeebzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Persisting the Model to Huggingface Hub  \n",
        "  \n",
        "After fine-tuning and validating our model, a optional step is to make it easily accessible for future use or sharing with the community. The Hugging Face Hub provides an excellent platform for this purpose.  \n",
        "  \n",
        "Uploading our model to the Hugging Face Hub offers several benefits:  \n",
        "1. Easy sharing and collaboration with other researchers or developers  \n",
        "2. Version control for your model iterations  \n",
        "3. Integration with various libraries and tools in the Hugging Face ecosystem  \n",
        "4. Simplified deployment options  \n",
        "  \n",
        "We'll demonstrate how to push our fine-tuned model and tokenizer to the Hugging Face Hub, making it available for others to use or for easy integration into other projects. This step is essential for reproducibility and for contributing to the broader NLP community.  \n",
        "\n",
        "**NOTE:** Check with your own legal counsel before pushing models to Huggingface Hub."
      ],
      "metadata": {
        "id": "se2YObHlj4f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "model.push_to_hub(\"rawkintrevo/granite-3b-code-instruct-2k-legal\",\n",
        "                  token= userdata.get('HF_TOKEN'))"
      ],
      "metadata": {
        "id": "JkL-yHtth3Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Fine-tuned Model  \n",
        "  \n",
        "Once we've saved our model, we can demonstrate how to load it back for inference. This step is crucial for real-world applications where you want to use your trained model without going through the training process again.  \n",
        "  \n",
        "Loading a saved model is typically much faster than training from scratch, making it efficient for deployment scenarios. We'll show how to load both the model and the tokenizer, ensuring that we have all the components necessary for text generation.  \n"
      ],
      "metadata": {
        "id": "IZFOrAj-IeRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# you would uncomment the next 3 lines to load in a new notebook\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"./results\")\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"./results\")"
      ],
      "metadata": {
        "id": "0roSm1wbee7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model from Huggingface Hub  \n",
        "  \n",
        "Once a model is pushed to the Hugging Face Hub, loading it for inference or further fine-tuning becomes remarkably straightforward. This ease of use is one of the key advantages of the Hugging Face ecosystem.  \n",
        "  \n",
        "We'll show how to load our fine-tuned model directly from the Hugging Face Hub using just a few lines of code. This process works not only for our own uploaded models but for any public model on the Hub, demonstrating the power and flexibility of this approach.  \n",
        "  \n",
        "Loading from the Hub allows you to:  \n",
        "1. Quickly experiment with different models  \n",
        "2. Easily integrate state-of-the-art models into your projects  \n",
        "3. Ensure you're using the latest version of a model  \n",
        "4. Access models from various devices or environments without needing to manually transfer files  \n",
        "  \n",
        "This capability is particularly useful in production environments, where you might need to dynamically load or update models based on specific requirements or performance metrics.  "
      ],
      "metadata": {
        "id": "3VarEb6jkFms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model = AutoModelForCausalLM.from_pretrained(\"rawkintrevo/granite-3b-code-instruct-2k-legal\")"
      ],
      "metadata": {
        "id": "wZ1oQO3UkKFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation  \n",
        "  \n",
        "Finally, we'll evaluate our fine-tuned model by presenting it with the same 'inheritance' prompt we used in the sanity check. This comparison will reveal how the model's understanding has shifted from a programming context to a legal one.  \n",
        "  \n",
        "This step demonstrates the power of transfer learning and domain-specific fine-tuning in natural language processing, showing how we can adapt a general-purpose language model to specialized tasks.  "
      ],
      "metadata": {
        "id": "VjhDEDYMHl_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"<|user>What does 'inheritance' mean?\\n<|assistant|>\\n\"\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=100)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "Byruf-ErUx2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Output\n",
        "\n",
        "```\n",
        "Inheritance refers to the right of a deceased person to inherit property. It is a legal concept that allows the deceased person's estate to be divided among their surviving family members. In the context of real estate, inheritance refers to the right of a deceased person to inherit their property. It is a legal concept that allows the deceased person's estate to be divided among their surviving family members. In the context of real estate, inheritance refers to\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "aXPG2FQzy9z0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Execution Times and Performance Metrics  \n",
        "  \n",
        "Throughout this notebook, we've been tracking the time taken for various stages of our process. These execution times provide valuable insights into the computational requirements of fine-tuning a large language model.  \n",
        "  \n",
        "We'll summarize the time taken for:  \n",
        "1. Loading the initial model  \n",
        "2. Performing the sanity check  \n",
        "3. Setting up the training environment  \n",
        "4. The actual training process  \n",
        "  \n",
        "Understanding these metrics is can be helpful for resource planning in machine learning projects. It helps in estimating the time and computational power needed for similar tasks in the future, and can guide decisions about hardware requirements or potential optimizations.  \n",
        "\n",
        "This topic is deep and nuanced, but this can give you an idea of how long your finetuning took on this particular hardware.\n",
        "  \n",
        "Additionally, we'll look at the training loss over time, which gives us a quantitative measure of how well our model learned from the legal dataset. This metric helps us gauge the effectiveness of our fine-tuning process.  "
      ],
      "metadata": {
        "id": "cxZHIvdbIoNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model Load Time: {model_loadtime} seconds\")\n",
        "print(f\"Model Sanity Check Time: {model_check_loadtime} seconds\")\n",
        "print(f\"Training Setup Time: {training_setup_loadtime} seconds\")\n",
        "print(f\"Training Time: {training_time} seconds ({training_time/60} minutes)\")"
      ],
      "metadata": {
        "id": "wkbvblKVzLiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Output\n",
        "\n",
        "```\n",
        "Model Load Time: 64.40367837800022 seconds\n",
        "Model Sanity Check Time: 9.231385502000194 seconds\n",
        "Training Setup Time: 4.85179586599952 seconds\n",
        "Training Time: 4826.068798849 seconds (80.43447998081666 minutes)\n",
        "```"
      ],
      "metadata": {
        "id": "gnppnMMK27SR"
      }
    }
  ]
}