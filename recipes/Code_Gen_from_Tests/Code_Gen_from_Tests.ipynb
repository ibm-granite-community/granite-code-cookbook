{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Code _from_ Your Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somewhat typical to want to generate unit tests from code, but when using _test-driven development_, you write a test _first_, then the code to make it _pass_. So, let's try doing that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "Like most of the other recipes, we will make inference calls against a model, IBM Granite 20b Code Instruct 8k in this case, that is hosted remotely on [Replicate](https://replicate.com/), hosted in the [ibm-granite](https://replicate.com/ibm-granite) organization. \n",
    "\n",
    "The notebook depends on the Granite [utils](https://github.com/ibm-granite-community/utils) package for integration with LLMs using the [Langchain](https://www.langchain.com/) framework.\n",
    "\n",
    "> **TIP:** See the [Getting Started with Replicate](https://github.com/ibm-granite-community/granite-kitchen/blob/main/recipes/Getting_Started/Getting_Started_with_Replicate.ipynb) notebook in the [granite-kitchen](https://github.com/ibm-granite-community/granite-kitchen) repo for more information about using Replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the required Langchain and Replicate packages\n",
    "\n",
    "Include a granite-community package with some simple utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/ibm-granite-community/utils \\\n",
    "    \"langchain_community<0.3.0\" \\\n",
    "    replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_granite_community.notebook_utils import get_env_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Code from a Hypothesis _Property-Based_ Test Suite\n",
    "\n",
    "We will use a testing library called [Hypothesis](https://hypothesis.readthedocs.io/en/latest/) that helps you think about the _properties_ the unit under test must satisfy. We will define some Hypothesis tests in a string below, then save it to a file so we can execute the tests. They cover a `Rational` class (for rational numbers) that doesn't exist yet. Finally, we will use Granite to generate an implementation of `Rational` that hopefully allows the tests to pass.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells install `hypothesis` and load the first set of tests for `Rational` from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install 'hypothesis[cli]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define our Hypothesis tests as a string and then save the string to a file so we can run the tests later. (Note that we have to use a hack to nest `\"\"\"` strings.)\n",
    "\n",
    "> **Note:** This test file is adapted from [this GitHub project](https://github.com/deanwampler/tdd-hypothesis-example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = '''\n",
    "# Example unit tests using Hypothesis for property-based testing.\n",
    "# Adapted from: https://github.com/deanwampler/tdd-hypothesis-example\n",
    "# Hypothesis website: https://hypothesis.readthedocs.io/en/latest/\n",
    "\n",
    "from hypothesis import given, strategies as st\n",
    "import unittest\n",
    "from rational import Rational\n",
    "from math import gcd\n",
    "\n",
    "class TestRational(unittest.TestCase):\n",
    "    '''\n",
    "    Test the features implemented currently by Rational.\n",
    "    Add new tests for Rational arithmetic operations, like multiplication and addition,\n",
    "    watch the test fail, then implement the feature and ensure the test now passes.\n",
    "    See also other properties described in the Rational Wikipedia page:\n",
    "    https://en.wikipedia.org/wiki/Rational_number\n",
    "    \n",
    "    Also, try adding a second way to construct Rationals that accepts a string\n",
    "    argument, \"M/N\". (Now you really have to think about handling input errors!) \n",
    "    What are the requirements for valid strings, e.g., for \"M\" and \"N\"?\n",
    "    If an invalid string is provided, how should the error be handled?\n",
    "    '''\n",
    "\n",
    "    # Disallow zero for the denominator!\n",
    "\n",
    "    nonzero_integers = st.integers().filter(lambda i: i != 0)\n",
    "\n",
    "    @given(st.integers(), nonzero_integers)\n",
    "    def test_init_takes_numerator_denominator(self, numer, denom):\n",
    "        '''\n",
    "        A \"relatively-trivial\" test, but note that the returned\n",
    "        numerator and denominator will be divided by their greatest\n",
    "        common divisor.\n",
    "        '''\n",
    "        rat = Rational(numer, denom)\n",
    "        divisor = gcd(numer, denom)\n",
    "        self.assertEqual(numer // divisor, rat.numerator)\n",
    "        self.assertEqual(denom // divisor, rat.denominator)\n",
    "\n",
    "    @given(st.integers())\n",
    "    def test_zero_denominator_raises(self, numer):\n",
    "        '''\n",
    "        Don't allow zero for the denominator!!\n",
    "        '''\n",
    "        with self.assertRaises(ValueError):\n",
    "            rat = Rational(numer, 0)\n",
    "\n",
    "    @given(st.integers(), nonzero_integers)\n",
    "    def test_a_rational_equals_itself(self, numer, denom):\n",
    "        '''\n",
    "        This test passes without adding a custom __eq__ method. \n",
    "        Without the __eq__ method, would this test actually use\n",
    "        \"logical\" instance equality or just locations in memory?\n",
    "        '''\n",
    "        rat = Rational(numer, denom)\n",
    "        self.assertEqual(rat, rat)\n",
    "\n",
    "    @given(st.integers(), nonzero_integers)\n",
    "    def test_identical_rationals_are_equal(self, numer, denom):\n",
    "        '''\n",
    "        Would this one pass if we deleted (or commented out) our custom __eq__ method? \n",
    "        Try it!\n",
    "        '''\n",
    "        rat1 = Rational(numer, denom)\n",
    "        rat2 = Rational(numer, denom)\n",
    "        self.assertEqual(rat1, rat2)\n",
    "\n",
    "    @given(st.integers(), nonzero_integers, nonzero_integers)\n",
    "    def test_equality_for_two_rationals_with_num_and_dom_that_are_multiples_of_each_other(self, numer, denom, multiple):\n",
    "        '''\n",
    "        Rule: a/b == c/d iff ad == bc\n",
    "        Since a*M/b*M == a/b, then a*M/b*M == c/d\n",
    "        '''\n",
    "        rat1 = Rational(numer*multiple, denom*multiple)\n",
    "        rat2 = Rational(numer, denom)\n",
    "        self.assertEqual(rat1, rat2)\n",
    "\n",
    "    @given(st.integers(), nonzero_integers, st.integers(), nonzero_integers)\n",
    "    def test_two_non_identical_rationals_are_not_equal_to_each_other(self, numer1, denom1, numer2, denom2):\n",
    "        '''\n",
    "        Rule: a/b == c/d iff ad == bc\n",
    "        This is a better test, because it randomly generates different instances.\n",
    "        However, the test has to check for the case where the two values happen to be\n",
    "        equivalent!\n",
    "        '''\n",
    "        rat1 = Rational(numer1, denom1)\n",
    "        rat2 = Rational(numer2, denom2)\n",
    "        if numer1*denom2 == numer2*denom1:\n",
    "            self.assertEqual(rat1, rat2)\n",
    "        else:\n",
    "            self.assertNotEqual(rat1, rat2)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    unittest.main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"test_rational.py\", mode=\"w+\") as f:  # save to a file.\n",
    "    f.write(tests)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the syntax briefly, the `@given` _decorators_ tell `hypothesis` to generate example values that will be passed as arguments to the test methods. \n",
    "\n",
    "For example, `test_init_takes_numerator_denominator` takes three parameters: `self` an integer numerator and a nonzero denominator. The `@given` will generate 100 examples of each value (by default) and call the test with combination of all those values. Note that this eliminates the need for you to generate a set of good examples yourself. The assertions in the test will verify that the expected logic is satisfied. \n",
    "\n",
    "See the [hypothesis](https://hypothesis.readthedocs.io/en/latest/) documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Generating an Implementation of `Rational`\n",
    "\n",
    "Let's now try to generate an implementation of `Rational` that passes the tests.\n",
    "\n",
    "First, we define a default _system prompt_ we'll pass as part of the inference call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_system_prompt = \"\"\"\n",
    "Role: Python Code Generator.\n",
    "User Input: Python tests, written using the hypothesis test library, https://hypothesis.readthedocs.io/en/latest/.\n",
    "Output: The Python code that implements the functionality in the Python hypothesis tests, so the tests pass. DO NOT print the tests in the output output. DO NOT print markdown or other separate documentation. DO print the Python code that makes the tests pass. DO add class and method documentation comments explaining what the code does, the arguments passed, etc.\n",
    "Validity: Only valid Python code is generated, which allows the input Python hypothesis tests to pass.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the model to use and a dictionary of parameters to pass to the `Replicate` constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Replicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"ibm-granite/granite-20b-code-instruct-8k\"\n",
    " \n",
    "input_parameters = {      \n",
    "        \"top_k\": 60,\n",
    "        \"top_p\": 0.3, \n",
    "        \"max_tokens\": 2000,\n",
    "        \"min_tokens\": 0,\n",
    "        \"temperature\": 0.3, \n",
    "        \"presence_penalty\": 0,\n",
    "        \"frequency_penalty\": 0,\n",
    "        \"system_prompt\": default_system_prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a `Replicate` instance, which makes a call to the Replicate service to authenticate.\n",
    "\n",
    "> **TIP:** If you get an authentication or similar error in the next cell, see the suggestions mentioned above about using Replicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "granite_via_replicate = Replicate(\n",
    "            model=model_id,\n",
    "            model_kwargs=input_parameters,\n",
    "            replicate_api_token=get_env_var('REPLICATE_API_TOKEN'),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Inference\n",
    "\n",
    "Finally, we invoke the model to generate `Rational` from the test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Here is the Hypothesis test code:\n",
    "```Python\n",
    "{tests}\n",
    "```\n",
    "\n",
    "Print Python code that makes the test code pass.\n",
    "\"\"\"\n",
    "\n",
    "replicate_response = granite_via_replicate.invoke(prompt)\n",
    "\n",
    "print(f\"Granite response from Replicate: {replicate_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find a good implementation of `Rational` [`rational-good-example.py`, in the GitHub repo](https://github.com/ibm-granite-community/granite-code-cookbook/blob/main/recipes/Code_Gen_from_Tests/rational-good-example.py).\n",
    "\n",
    "How closely does the Granite output match this code? Does it appear to satisfy the requirements for rationals in the [Wikipedia article](https://en.wikipedia.org/wiki/Rational_number)? \n",
    "\n",
    "If important logic is missing, try changing the `prompt`, which is currently very generic:\n",
    "1. Make the prompt more specific about the properties of rational numbers.\n",
    "2. Add the link to the Wikipedia page mentioned above (which is also mentioned in the test code comments).\n",
    "\n",
    "Try any changes to the prompt one at a time to see their relative impact. Also, it's useful to run the query several times for each prompt change to see how the results vary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the Tests Pass??\n",
    "\n",
    "Let's run the tests! Execute the next cell to create a `rational` directory and files we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf rational\n",
    "!mkdir -p rational\n",
    "!echo \"from .rational import Rational\" > rational/__init__.py\n",
    "!ls -al rational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the test code expects to find a `Rational` type in the `rational` package, which is why we did some of the steps in the previous cell.\n",
    "\n",
    "Now copy the generated `Rational` class definition in the output above and paste it in between the `\"\"\"` quotes in the next cell. _If the generated code has `\"\"\"` comments_, either delete them, replace them with `'''` (triple single quotes), or add a `\\` in front of the first of the three `\"` _for every case_, like we did for the tests above.\n",
    "\n",
    "**Don't** include the test code, any markdown, or other text that was part of the generated output!\n",
    "\n",
    "**Do** include appropriate `import` statements, e.g., you may see that the generated code calls `gcd` (_greatest common divisor_), which would require this import `from math import gcd`. (If you aren't sure why `gcd` is useful, see the Wikipedia article linked above about the properties of rational numbers.)\n",
    "\n",
    "Is the indentation correct in the code? Make sure each level is properly indented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rational = \"\"\"\n",
    "from math import gcd\n",
    "\n",
    "class Rational:\n",
    "    def __init__(self, numerator, denominator):\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Cannot create a Rational with a zero denominator.\")\n",
    "\n",
    "        divisor = gcd(numerator, denominator)\n",
    "        self.numerator = numerator // divisor\n",
    "        self.denominator = denominator // divisor\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.numerator}/{self.denominator}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.numerator * other.denominator == self.denominator * other.numerator\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, create a new `rational/rational.py` file to hold your generated `Rational` code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rational/rational.py\", mode=\"w+\") as f:  # save to a file.\n",
    "    f.write(rational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -al rational"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're finally ready to run the tests, by running the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./test_rational.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did the tests pass? If not, what can you change in the generated `Rational` code to make them pass. For example, compare the generated code to the [`rational-good-example.py` implementation in the GitHub repo](https://github.com/ibm-granite-community/granite-code-cookbook/blob/main/recipes/Code_Gen_from_Tests/rational-good-example.py) we mentioned above. Can you modify the prompt to generate these improvements? \n",
    "\n",
    "For comparison and convenience, here is `rational-good-example.py`, stripped of comments. If you try using this code instead for the definition of the `rational` variable above, the tests should pass.\n",
    "\n",
    "```python\n",
    "rational = '''\n",
    "from math import gcd\n",
    "\n",
    "class Rational:\n",
    "    def __init__(self, numerator, denominator):\n",
    "        if denominator == 0:\n",
    "            raise ValueError(\"Cannot create a Rational with a zero denominator.\")\n",
    "\n",
    "        divisor = gcd(numerator, denominator)\n",
    "        self.numerator = numerator // divisor\n",
    "        self.denominator = denominator // divisor\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"{self.numerator}/{self.denominator}\"\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self.numerator * other.denominator == self.denominator * other.numerator\n",
    "'''\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Additional Practice\n",
    "\n",
    "1. Read more about [Hypothesis](https://hypothesis.readthedocs.io/en/latest/) and _property-based_ testing, a powerful technique for structuring your tests, and designing the corresponding code, in a rigorous way, and not just for mathematical types, like `Rational`.\n",
    "2. Add new tests to `test_rational.py` for other operators, like `*`, `/`, `+`, `-`, `<`, `<=`, `>`, `>=`, then regenerate `Rational` and see if the implementations of these operators are properly generated. Keep in mind that `Rational(N*numerator, N*denominator)`, for some integer `N`, numerator `numerator`, and denominator `denominator`, is always \"rationalized\" to `Rational(numerator, denominator)`.\n",
    "3. Try writing a new test suite for a `Complex` number type and see how well an implementation for it is generated."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
